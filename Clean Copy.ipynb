{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33bc10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools as ft\n",
    "#remove repeats\n",
    "from datetime import date,datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "798d9201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_reservations(df):\n",
    "    try:\n",
    "        Fab300_raw_reservations = df.copy()\n",
    "        SortedRows = Fab300_raw_reservations.sort_values([\"EVENT_ROW_ID\"])\n",
    "\n",
    "        index = range(1,len(SortedRows)+1)\n",
    "        IndexShift_1 = [i-1 if i%2 != 0 else np.nan for i in index]\n",
    "        IndexShift_2 = [i-1 if i%2 == 0 else np.nan for i in index]\n",
    "\n",
    "        SortedRows[\"Index\"] = index\n",
    "        SortedRows[\"IndexShift_1\"] = IndexShift_1\n",
    "        SortedRows[\"IndexShift_2\"] = IndexShift_2\n",
    "        SortedRows[\"IndexShift_1_1\"] = SortedRows[\"IndexShift_1\"].fillna(method='bfill')\n",
    "        SortedRows[\"IndexShift_2_1\"] = SortedRows[\"IndexShift_2\"].fillna(method='bfill')\n",
    "        SortedRows = SortedRows.drop(columns=['IndexShift_1', 'IndexShift_2'])\n",
    "        SortedRows = SortedRows.rename(columns={\"IndexShift_1_1\":\"IndexShift_1\",\"IndexShift_2_1\":\"IndexShift_2\"})\n",
    "        FilledUp2 = SortedRows.copy()\n",
    "\n",
    "        UnpivotedOnlySelectedColumns = pd.melt(FilledUp2, id_vars=['FO_ROW_ID','Index','IndexShift_1','IndexShift_2'], \n",
    "                    value_vars=[\"ResWBS\", \"ResTk\", \"DATE_TIME_STAMP\", \"USER_ID\", \"EVENT_ROW_ID\"])\n",
    "\n",
    "        UnpivotedOnlySelectedColumns[\"ResProperty_1\"] = np.where(UnpivotedOnlySelectedColumns['Index']==UnpivotedOnlySelectedColumns['IndexShift_1'], UnpivotedOnlySelectedColumns['variable'] + \"_Start\", UnpivotedOnlySelectedColumns['variable'] + \"_End\")\n",
    "        UnpivotedOnlySelectedColumns[\"ResProperty_2\"] = np.where(UnpivotedOnlySelectedColumns['Index']==UnpivotedOnlySelectedColumns['IndexShift_2'], UnpivotedOnlySelectedColumns['variable'] + \"_Start\", UnpivotedOnlySelectedColumns['variable'] + \"_End\")\n",
    "\n",
    "        AddedCustom4 = UnpivotedOnlySelectedColumns.copy()\n",
    "        RemovedColumns1 = AddedCustom4.drop(columns=[\"Index\", \"IndexShift_2\", \"variable\", \"ResProperty_2\"])\n",
    "\n",
    "        PivotedColumn1 = RemovedColumns1.pivot(index=['FO_ROW_ID','IndexShift_1'],columns='ResProperty_1',values='value').reset_index()\n",
    "\n",
    "        if \"ResWBS_Start\" in PivotedColumn1.columns:\n",
    "            FilteredRows01 = PivotedColumn1[(PivotedColumn1[\"ResWBS_Start\"].notnull() & PivotedColumn1['ResWBS_Start'].str.len() > 0)]\n",
    "        else:\n",
    "            #empty dataframe\n",
    "            FilteredRows01 = pd.DataFrame(columns=PivotedColumn1.columns)\n",
    "            \n",
    "        \n",
    "        RemovedColumns2 = AddedCustom4.drop(columns=[\"Index\", \"IndexShift_1\", \"variable\", \"ResProperty_1\"])\n",
    "        PivotedColumn2 = RemovedColumns2.pivot(index=['FO_ROW_ID','IndexShift_2'],columns='ResProperty_2',values='value').reset_index()\n",
    "\n",
    "        FilteredRows02 = PivotedColumn2[(PivotedColumn2[\"ResWBS_Start\"].notnull() & PivotedColumn2['ResWBS_Start'].str.len() > 0)]\n",
    "        \n",
    "        if \"ResWBS_Start\" in PivotedColumn2.columns:\n",
    "            FilteredRows02 = PivotedColumn2[(PivotedColumn2[\"ResWBS_Start\"].notnull() & PivotedColumn2['ResWBS_Start'].str.len() > 0)]\n",
    "        else:\n",
    "            #empty dataframe\n",
    "            FilteredRows02 = pd.DataFrame(columns=PivotedColumn2.columns)\n",
    "        \n",
    "        columns = [\n",
    "            'FO_ROW_ID', \n",
    "            'DATE_TIME_STAMP_End',\n",
    "            'DATE_TIME_STAMP_Start', \n",
    "            'EVENT_ROW_ID_End', \n",
    "            'EVENT_ROW_ID_Start',\n",
    "            'ResTk_End', \n",
    "            'ResTk_Start', \n",
    "            'ResWBS_End', \n",
    "            'ResWBS_Start', \n",
    "            'USER_ID_End',\n",
    "            'USER_ID_Start'\n",
    "        ]\n",
    "            \n",
    "            \n",
    "        if FilteredRows01.shape[0] == 0:\n",
    "            combined = FilteredRows02[columns]\n",
    "        elif FilteredRows02.shape[0] == 0:\n",
    "            combined = FilteredRows01[columns]\n",
    "        else:\n",
    "            FilteredRows01 = FilteredRows01[columns]\n",
    "            FilteredRows02 = FilteredRows02[columns]\n",
    "        \n",
    "        combined = pd.concat([FilteredRows01,FilteredRows02])\n",
    "\n",
    "        combined = combined.rename(\n",
    "            columns=\n",
    "            {\n",
    "                \"ResTk_Start\":\"ResTk\",\n",
    "                \"ResWBS_Start\":\"WBS\",\n",
    "                \"EVENT_ROW_ID_Start\": \"EVENT_ROW_ID_Begin\",\n",
    "                \"DATE_TIME_STAMP_Start\": \"DATE_TIME_STAMP_Begin\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        columns = [\n",
    "            \"FO_ROW_ID\",\n",
    "            \"ResTk\", \n",
    "            \"WBS\", \n",
    "            \"EVENT_ROW_ID_Begin\", \n",
    "            \"EVENT_ROW_ID_End\", \n",
    "            \"DATE_TIME_STAMP_Begin\", \n",
    "            \"DATE_TIME_STAMP_End\", \n",
    "            \"USER_ID_Start\", \n",
    "            \"USER_ID_End\"\n",
    "        ]\n",
    "        Tools_with_resersvations  = combined#[columns]\n",
    "\n",
    "        final = combined[columns]\n",
    "        return final\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def processFab300RawReservations(df):\n",
    "    columns = [\n",
    "        \"FO_ROW_ID\",\n",
    "        \"DATE_TIME_STAMP_Begin\",\n",
    "        \"DATE_TIME_STAMP_End\",\n",
    "        'EVENT_ROW_ID_Begin', \n",
    "        'EVENT_ROW_ID_End',\n",
    "        'ResTk',\n",
    "        'USER_ID_Start',\n",
    "        'USER_ID_End',\n",
    "        'WBS'\n",
    "    ]\n",
    "\n",
    "    for_row_ids = df[\"FO_ROW_ID\"].unique()\n",
    "\n",
    "    Tools_with_reservations = pd.DataFrame(columns=columns)\n",
    "    for row_id in for_row_ids:\n",
    "        grpdata = df[df[\"FO_ROW_ID\"] == row_id]\n",
    "\n",
    "        df_reserv = identify_reservations(grpdata)\n",
    "        if df.shape[0] > 0:\n",
    "            Tools_with_reservations = pd.concat([Tools_with_reservations,df_reserv])\n",
    "\n",
    "\n",
    "    return Tools_with_reservations\n",
    "\n",
    "\n",
    "def FAB300_with_tool_names(Tools_with_reservations,Tools_Parents):\n",
    "    Expanded_Tools_parents = pd.merge(\n",
    "        Tools_with_reservations, \n",
    "        Tools_Parents, \n",
    "        left_on=[\"FO_ROW_ID\"], \n",
    "        right_on=[\"ROW_ID\"], \n",
    "        how=\"left\",\n",
    "        suffixes=[\"\",\"_y\"]\n",
    "    )\n",
    "    Expanded_Tools_parents = Expanded_Tools_parents.rename(\n",
    "        columns={\n",
    "            \"ENT_NAME\":\"Tool\",\n",
    "            \"USER_ID_Start\":\"USER_ID_Begin\",\n",
    "            \"USER_ID_End\":\"USER_ID_End\"\n",
    "        }\n",
    "    )\n",
    "    Expanded_Tools_parents_filteredRows = Expanded_Tools_parents[\n",
    "        Expanded_Tools_parents[\"USER_ID_Begin\"] == Expanded_Tools_parents[\"USER_ID_End\"]\n",
    "    ]\n",
    "    Expanded_Tools_parents_filteredRows = Expanded_Tools_parents_filteredRows.rename(\n",
    "        columns={\n",
    "            \"USER_ID_Begin\":\"User_id\",\n",
    "            \"DATE_TIME_STAMP_Begin\":\"Begin\",\n",
    "            \"DATE_TIME_STAMP_End\":\"End\"\n",
    "        }\n",
    "    )\n",
    "    Expanded_Tools_parents_filteredRows = Expanded_Tools_parents_filteredRows.sort_values([\"FO_ROW_ID\",\"EVENT_ROW_ID_Begin\"])\n",
    "    Fab300_Res_id = range(0,len(Expanded_Tools_parents_filteredRows))\n",
    "    Expanded_Tools_parents_filteredRows[\"Fab300_Res_id\"] = Fab300_Res_id\n",
    "    Expanded_Tools_parents_filteredRows = Expanded_Tools_parents_filteredRows[\n",
    "        (Expanded_Tools_parents_filteredRows[\"Tool\"].notnull())\n",
    "        & (Expanded_Tools_parents_filteredRows['Tool'].str.len() > 0)\n",
    "    ]\n",
    "    columns = [\n",
    "        \"FO_ROW_ID\",\n",
    "        \"EVENT_ROW_ID_Begin\",\n",
    "        \"Begin\",\n",
    "        \"End\",\n",
    "        \"Fab300_Res_id\",\n",
    "        \"FACILITY\",\n",
    "        \"ResTk\",\n",
    "        \"Tool\",\n",
    "        \"User_id\",\n",
    "        \"WBS\"\n",
    "    ]\n",
    "    Expanded_Tools_parents_filteredRows[columns]\n",
    "    FAB300_with_tool_names = Expanded_Tools_parents_filteredRows[columns]\n",
    "\n",
    "    #Sample for FO_ROW_ID == 76\n",
    "    return FAB300_with_tool_names\n",
    "\n",
    "\n",
    "#IIO_raw_reservations\n",
    "def IIO_without_modules(df):\n",
    "    df = df.reset_index()\n",
    "    df = df.sort_values([\"WBS\",\"Tool\",\"FACILITY\",\"Module\",\"Begin\"])\n",
    "    df[\"End_Down\"] = df[\"End\"].shift(1)\n",
    "    df[\"Adjacent_Down\"] = np.where(df['Begin']==df['End_Down'], True, False)\n",
    "    df[\"IndexCopy\"] = np.where(df[\"Adjacent_Down\"] == True,np.nan,df[\"index\"])\n",
    "    df[\"IndexCopy2\"] = df[\"IndexCopy\"].fillna(method='ffill')\n",
    "    df[\"Module\"] = df[\"Module\"].fillna(\"\")\n",
    "    df[\"Description\"] = df[\"Description\"].fillna(\"\")\n",
    "    df = df[['WBS','FACILITY', 'Module','Tool','Begin', 'Description', 'End', 'User_id','IndexCopy2','End_Down']]\n",
    "\n",
    "\n",
    "    params = {\n",
    "        'Begin': 'min',\n",
    "        'End': 'max',\n",
    "        'Description': lambda x: ';'.join(sorted(pd.Series.unique(x))),\n",
    "        'User_id': lambda x: ';'.join(sorted(pd.Series.unique(x)))\n",
    "    }\n",
    "    sub = df[[\"WBS\",\"Tool\",\"FACILITY\",\"Module\",\"Begin\",\"End\",\"Description\",'User_id','IndexCopy2']]\n",
    "    GroupedRows1 = sub.groupby([\"WBS\",\"Tool\",\"FACILITY\",\"Module\",'IndexCopy2']).agg(params).reset_index()\n",
    "\n",
    "    params = {\n",
    "        'Module': lambda x: ';'.join(sorted(pd.Series.unique(x))),\n",
    "        'Description': 'first',\n",
    "        'User_id': 'first'\n",
    "    }\n",
    "    \n",
    "\n",
    "    GroupedRows1 = GroupedRows1.groupby([\"WBS\",\"Tool\",\"FACILITY\",\"Begin\", \"End\"]).agg(params).reset_index()\n",
    "    GroupedRows1 = GroupedRows1.sort_values([\"FACILITY\", \"Tool\", \"WBS\", \"Begin\", \"End\"])\n",
    "\n",
    "    Index = range(0,len(GroupedRows1))\n",
    "    GroupedRows1[\"IIO_Res_id\"] = Index\n",
    "    \n",
    "    GroupedRows1 = GroupedRows1.rename(\n",
    "        columns={\n",
    "            \"Module\":\"Modules\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    IIO_without_modules = GroupedRows1.copy()\n",
    "    return IIO_without_modules\n",
    "\n",
    "\n",
    "def Fab300_iio_merger(df,verbose=False):\n",
    "    df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])\n",
    "    df = df.sort_values([\"WBS\",\"FACILITY\",'Tool',\"DateTime\",\"FAB300_BeginEnd\"])\n",
    "    \n",
    "    df[\"Fab300_Res_id_UP\"] = df[\"Fab300_Res_id\"].fillna(method='bfill')\n",
    "    df[\"Fab300_Res_id_DOWN\"] = df[\"Fab300_Res_id\"].fillna(method='ffill')\n",
    "    df[\"IIO_Res_id_UP\"] = df[\"IIO_Res_id\"].fillna(method='bfill')\n",
    "    df[\"IIO_Res_id_DOWN\"] = df[\"IIO_Res_id\"].fillna(method='ffill')\n",
    "    \n",
    "    if verbose:\n",
    "        print(df.shape)\n",
    "        \n",
    "    fab_iio_Filtered_Rows = df[\n",
    "        (df[\"Fab300_Res_id_UP\"] == df[\"Fab300_Res_id_DOWN\"])\n",
    "        & (df[\"IIO_Res_id_UP\"] == df[\"IIO_Res_id_DOWN\"])\n",
    "        & (df[\"Fab300_Res_id_UP\"].notnull())\n",
    "        & (df[\"IIO_Res_id_UP\"].notnull())\n",
    "    ] \n",
    "    if verbose:\n",
    "        print(fab_iio_Filtered_Rows.shape)\n",
    "        \n",
    "    Removed_Other_Columns = fab_iio_Filtered_Rows[[\"Fab300_Res_id_UP\", \"IIO_Res_id_UP\"]]\n",
    "    Removed_Duplicates = Removed_Other_Columns.drop_duplicates()\n",
    "    Renamed_Columns = Removed_Duplicates.rename(\n",
    "        columns = {\n",
    "            \"Fab300_Res_id_UP\":\"Fab300_Res_id\",\n",
    "            \"IIO_Res_id_UP\":\"IIO_Res_id\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    index = range(0,len(Renamed_Columns))\n",
    "    Renamed_Columns[\"Index\"] = index\n",
    "    Added_Index = Renamed_Columns.copy()\n",
    "    State_DOWN_Remove = Added_Index.copy()\n",
    "    State_DOWN_Remove.loc[-1] = [np.nan,np.nan,-1]  # adding a row\n",
    "    State_DOWN_Insert = State_DOWN_Remove.copy()\n",
    "    State_DOWN_Insert = State_DOWN_Insert.sort_values([\"Index\"])\n",
    "    index2 = range(0,len(State_DOWN_Insert))\n",
    "    State_DOWN_Insert[\"Index2\"] = index2\n",
    "\n",
    "    State_DOWN_Add_Index = State_DOWN_Insert.copy()\n",
    "    State_DOWN_Rename = State_DOWN_Add_Index.rename(\n",
    "        columns = {\n",
    "            \"Fab300_Res_id\":\"Fab300_Res_id_DOWN\",\n",
    "            \"IIO_Res_id\":\"IIO_Res_id_DOWN\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    With_DOWN = Added_Index.merge(\n",
    "        State_DOWN_Rename, \n",
    "        left_on=[\"Index\"],\n",
    "        right_on=[\"Index2\"],\n",
    "        suffixes=[\"\",\"_y\"],\n",
    "        how = 'left'\n",
    "    )\n",
    "\n",
    "    With_DOWN[\"Index3\"] = np.where(\n",
    "        (With_DOWN['Fab300_Res_id']==With_DOWN['Fab300_Res_id_DOWN'])\n",
    "        | (With_DOWN['IIO_Res_id']==With_DOWN['IIO_Res_id_DOWN']), \n",
    "        np.nan,\n",
    "         With_DOWN['Index']\n",
    "    )\n",
    "\n",
    "\n",
    "    Replaced_Value = With_DOWN.copy()\n",
    "    Replaced_Value[\"Index4\"] = Replaced_Value[\"Index3\"].fillna(method='ffill')\n",
    "    Replaced_Value = Replaced_Value.drop(columns=[\"Fab300_Res_id_DOWN\", \"IIO_Res_id_DOWN\"])\n",
    "    Renamed_Columns1 = Replaced_Value.rename(\n",
    "    columns={\n",
    "        \"Index4\":\"Cluster\"\n",
    "    })\n",
    "\n",
    "\n",
    "    Merged_queries = Renamed_Columns1.merge(\n",
    "        Source_fab, \n",
    "        left_on=[\"Fab300_Res_id\"],\n",
    "        right_on=[\"Fab300_Res_id\"],\n",
    "        suffixes=[\"\",\"_y\"],\n",
    "        how = 'left'\n",
    "    )\n",
    "    Expanded_Fab300_with_tool_names = Merged_queries.rename(\n",
    "    columns ={\n",
    "        \"Begin\": \"Fab300_Begin\", \n",
    "        \"End\": \"Fab300_End\", \n",
    "        \"User_id\": \"Fab300_User_id\"\n",
    "    })\n",
    "    Merged_queries_1 = Expanded_Fab300_with_tool_names.merge(\n",
    "        Source_iio, \n",
    "        left_on=[\"IIO_Res_id\"],\n",
    "        right_on=[\"IIO_Res_id\"],\n",
    "        suffixes=[\"\",\"_y\"],\n",
    "        how = 'left'\n",
    "    )\n",
    "    Expanded_IIO_without_modules = Merged_queries_1.rename(\n",
    "    columns ={\n",
    "        \"Begin\": \"IIO_Begin\", \n",
    "        \"End\": \"IIO_End\", \n",
    "        \"Modules\": \"Modules\",\n",
    "        \"User_id\": \"IIO_User_id\",\n",
    "        \"Description\": \"Description\"\n",
    "    })\n",
    "    Expanded_IIO_without_modules = Expanded_IIO_without_modules.drop(\n",
    "        columns=[\"FACILITY_y\",\"Tool_y\",\"WBS_y\",'Index', 'Index_y', 'Index2', 'Index3']\n",
    "    )\n",
    "    \n",
    "    return Expanded_IIO_without_modules\n",
    "\n",
    "\n",
    "def Final_Fab300_IIO_reservations(IIO_without_modules,Fab300withtoolnames,Fab300_IIO_overlaps_ids):\n",
    "    Fab300_IIO_overlaps_ids[\"Fab300_Begin\"] = pd.to_datetime(Fab300_IIO_overlaps_ids[\"Fab300_Begin\"],format=\"%Y-%m-%d %H:%M\")\n",
    "    Fab300_IIO_overlaps_ids[\"Fab300_End\"] = pd.to_datetime(Fab300_IIO_overlaps_ids[\"Fab300_End\"],format=\"%Y-%m-%d %H:%M\")\n",
    "    Fab300_IIO_overlaps_ids[\"IIO_Begin\"] = pd.to_datetime(Fab300_IIO_overlaps_ids[\"IIO_Begin\"],format=\"%Y-%m-%d %H:%M\")\n",
    "    Fab300_IIO_overlaps_ids[\"IIO_End\"] = pd.to_datetime(Fab300_IIO_overlaps_ids[\"IIO_End\"],format=\"%Y-%m-%d %H:%M\")\n",
    "    \n",
    "    AddedCustom = Fab300_IIO_overlaps_ids.copy()\n",
    "\n",
    "    AddedCustom['Modules'] = AddedCustom['Modules'].fillna('')\n",
    "    AddedCustom[\"Fab300_Duration\"] = (AddedCustom[\"Fab300_End\"] - AddedCustom[\"Fab300_Begin\"])/np.timedelta64(1, 'h')\n",
    "    AddedCustom1 = AddedCustom.copy()\n",
    "    AddedCustom1[\"IIO_Duration\"] = (AddedCustom1[\"IIO_End\"] - AddedCustom1[\"IIO_Begin\"])/np.timedelta64(1, 'h')\n",
    "    params = {\n",
    "        'Fab300_Begin': 'min',\n",
    "        'IIO_Begin': 'min',\n",
    "        'Fab300_End': 'max',\n",
    "        'IIO_End': 'max',\n",
    "        'Fab300_Res_id': 'count',\n",
    "        'Description': lambda x: ';'.join(sorted(pd.Series.unique(x))),\n",
    "        'IIO_User_id': lambda x: ';'.join(sorted(pd.Series.unique(x))),\n",
    "        'Fab300_User_id': lambda x: ';'.join(sorted(pd.Series.unique(x))),\n",
    "        'Modules': lambda x: ';'.join(sorted(pd.Series.unique(x)))\n",
    "    }\n",
    "    sub = AddedCustom1[\n",
    "        [\n",
    "            \"FACILITY\", \n",
    "            \"Tool\", \n",
    "            \"WBS\", \n",
    "            \"Cluster\",\n",
    "            \"Fab300_Begin\",\n",
    "            \"IIO_Begin\",\n",
    "            'Fab300_End',\n",
    "            'IIO_End',\n",
    "            'Fab300_Res_id',\n",
    "            'IIO_Res_id',\n",
    "            'Description',\n",
    "            'IIO_User_id',\n",
    "            'Fab300_User_id',\n",
    "            'Modules'\n",
    "        ]\n",
    "    ]\n",
    "    GroupedRows1 = sub.groupby([\"FACILITY\", \"Tool\", \"WBS\", \"Cluster\"]).agg(params).reset_index()\n",
    "    GroupedRows1[\"Begin\"] = np.where(GroupedRows1['Fab300_Begin']>GroupedRows1['IIO_Begin'], GroupedRows1['IIO_Begin'], GroupedRows1['Fab300_Begin'])\n",
    "    GroupedRows1[\"End\"]  = np.where(GroupedRows1['Fab300_End']>GroupedRows1['IIO_End'], GroupedRows1['Fab300_End'], GroupedRows1['IIO_End'])\n",
    "    GroupedRows1 = GroupedRows1.rename(\n",
    "    columns={\n",
    "        \"Fab300_Res_id\": \"Cnt\"\n",
    "    }).drop(columns=['Fab300_Begin','Fab300_End','IIO_Begin','IIO_End'])\n",
    "\n",
    "\n",
    "    Duration_FAB = AddedCustom1[\n",
    "        [\n",
    "            \"FACILITY\", \n",
    "            \"Tool\", \n",
    "            \"WBS\", \n",
    "            \"Cluster\",\n",
    "            \"Fab300_Res_id\",\n",
    "            'Fab300_Duration'\n",
    "        ]\n",
    "    ].drop_duplicates().reset_index()\n",
    "\n",
    "    Duration_IIO = AddedCustom1[\n",
    "        [\n",
    "            \"FACILITY\", \n",
    "            \"Tool\", \n",
    "            \"WBS\", \n",
    "            \"Cluster\",\n",
    "            \"IIO_Res_id\",\n",
    "            'IIO_Duration'\n",
    "        ]\n",
    "    ].drop_duplicates().reset_index()\n",
    "\n",
    "    GroupedRows1_FAB = Duration_FAB.groupby([\"FACILITY\", \"Tool\", \"WBS\", \"Cluster\"]).sum().reset_index()\n",
    "    GroupedRows1_IIO = Duration_IIO.groupby([\"FACILITY\", \"Tool\", \"WBS\", \"Cluster\"]).sum().reset_index()\n",
    "\n",
    "\n",
    "    GroupedRows = pd.concat(\n",
    "        objs=(iDF.set_index([\"FACILITY\", \"Tool\", \"WBS\", \"Cluster\"]) for iDF in (GroupedRows1, GroupedRows1_FAB, GroupedRows1_IIO)),\n",
    "        axis=1, \n",
    "        join='inner'\n",
    "    ).reset_index()\n",
    "    GroupedRows['FAB_IIO_Ratio'] = GroupedRows[\"Fab300_Duration\"] / GroupedRows[\"IIO_Duration\"]\n",
    "\n",
    "    GroupedRows = GroupedRows[\n",
    "        [\n",
    "            'FACILITY', \n",
    "            'Tool', \n",
    "            'WBS', \n",
    "            'Cluster', \n",
    "            'Cnt', \n",
    "            'Description',\n",
    "            'Begin', \n",
    "            'End',\n",
    "            'IIO_User_id', \n",
    "            'Fab300_User_id', \n",
    "            'Modules',\n",
    "            'Fab300_Duration', \n",
    "            'IIO_Duration', \n",
    "            'FAB_IIO_Ratio'\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    FilteredRows = GroupedRows[\n",
    "        (GroupedRows[\"FAB_IIO_Ratio\"] < 3.0)\n",
    "        & (GroupedRows[\"Cnt\"] <= 4)\n",
    "    ]\n",
    "\n",
    "    Fab300_IIO_valid_combos = FilteredRows.drop(columns = [\"Cnt\"])\n",
    "\n",
    "    outer = Fab300_IIO_overlaps_ids.merge(\n",
    "        Fab300_IIO_valid_combos, \n",
    "        how='outer',\n",
    "        left_on = [\"FACILITY\", \"Tool\", \"WBS\", \"Cluster\"],\n",
    "        right_on = [\"FACILITY\", \"Tool\", \"WBS\", \"Cluster\"],\n",
    "        indicator=True\n",
    "    )\n",
    "    Fab300_IIO_bad_combos = outer[(outer._merge=='left_only')].drop('_merge', axis=1)\n",
    "\n",
    "\n",
    "    IIO_ids_from_bad_combos = Fab300_IIO_bad_combos[[\"IIO_Res_id\"]]\n",
    "    RemovedDuplicates = IIO_ids_from_bad_combos.drop_duplicates()\n",
    "\n",
    "    MergedQueries = RemovedDuplicates.merge(\n",
    "        IIO_without_modules, \n",
    "        how='inner',\n",
    "        left_on = [\"IIO_Res_id\"],\n",
    "        right_on = [\"IIO_Res_id\"],\n",
    "        indicator=True\n",
    "    )\n",
    "    RemovedColumns1 = MergedQueries.drop(columns=[\"IIO_Res_id\"])\n",
    "    IIO_from_bad_combos = RemovedColumns1.rename(\n",
    "    columns = {\n",
    "        'User_id':'IIO_User_id'\n",
    "    }).drop(columns=[\"_merge\"])\n",
    "\n",
    "    outer_IIO_from_bad_combos = IIO_without_modules.merge(\n",
    "        Fab300_IIO_overlaps_ids, \n",
    "        how='outer',\n",
    "        left_on = [\"IIO_Res_id\"],\n",
    "        right_on = [\"IIO_Res_id\"],\n",
    "        indicator=True\n",
    "    )\n",
    "    Pure_IIO_Join = outer_IIO_from_bad_combos[(outer_IIO_from_bad_combos._merge=='left_only')].drop('_merge', axis=1)\n",
    "    Pure_IIO = Pure_IIO_Join[\n",
    "        [\n",
    "            'Begin', \n",
    "            'Description_x', \n",
    "            'End', \n",
    "            'FACILITY_x', \n",
    "            'IIO_Res_id',\n",
    "            'Modules_x', \n",
    "            'Tool_x', \n",
    "            'User_id', \n",
    "            'WBS_x'\n",
    "        ]\n",
    "    ]\n",
    "    RenamedColumns = Pure_IIO.rename(\n",
    "        columns = {\n",
    "            'User_id':'IIO_User_id',\n",
    "            'Description_x': 'Description',\n",
    "            'FACILITY_x': 'FACILITY',\n",
    "            'Modules_x': 'Modules',\n",
    "            'Tool_x': 'Tool',\n",
    "            'WBS_x': 'WBS'\n",
    "        }\n",
    "    )\n",
    "    AppendedQuery = pd.concat([RenamedColumns,IIO_from_bad_combos]).drop(columns=['IIO_Res_id'])\n",
    "    AppendedQuery[\"Begin\"] = pd.to_datetime(AppendedQuery[\"Begin\"],format=\"%Y-%m-%d %H:%M\")\n",
    "    AppendedQuery[\"End\"] = pd.to_datetime(AppendedQuery[\"End\"],format=\"%Y-%m-%d %H:%M\")\n",
    "    AppendedQuery[\"IIO_Duration\"] = (AppendedQuery[\"End\"] - AppendedQuery[\"Begin\"])/np.timedelta64(1, 'h')\n",
    "    AddedCustom2 = AppendedQuery.copy()\n",
    "    AppendedQuery2 = pd.concat([Fab300_IIO_valid_combos,AddedCustom2])\n",
    "\n",
    "    outer_Fab300withtoolnames = Fab300withtoolnames.merge(\n",
    "        Fab300_IIO_overlaps_ids, \n",
    "        how='outer',\n",
    "        left_on = [\"Fab300_Res_id\"],\n",
    "        right_on = [\"Fab300_Res_id\"],\n",
    "        indicator=True\n",
    "    )\n",
    "\n",
    "    Pure_Fab300_Join = outer_Fab300withtoolnames[(outer_Fab300withtoolnames._merge=='left_only')].drop('_merge', axis=1)\n",
    "    Pure_Fab300 = Pure_Fab300_Join[\n",
    "        [\n",
    "            'Begin', \n",
    "            'End', \n",
    "            'Fab300_Res_id', \n",
    "            'FACILITY_x', \n",
    "            'Tool_x',\n",
    "            'User_id',\n",
    "            'WBS_x'\n",
    "        ]\n",
    "    ].drop(columns=['Fab300_Res_id']).rename(\n",
    "    columns={\n",
    "            'FACILITY_x': 'FACILITY',\n",
    "            'Tool_x': 'Tool',\n",
    "            'User_id': 'Fab300_User_id',\n",
    "            'WBS_x': 'WBS'\n",
    "    })\n",
    "    RenamedColumns1 = Pure_Fab300.copy()\n",
    "\n",
    "    RenamedColumns1[\"Begin\"] = pd.to_datetime(RenamedColumns1[\"Begin\"],format=\"%Y-%m-%d %H:%M\")\n",
    "    RenamedColumns1[\"End\"] = pd.to_datetime(RenamedColumns1[\"End\"],format=\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    RenamedColumns1[\"Fab300_Duration\"] = (RenamedColumns1[\"End\"] - RenamedColumns1[\"Begin\"])/np.timedelta64(1, 'h')\n",
    "    AddedCustom3 = RenamedColumns1.copy()\n",
    "\n",
    "    AppendedQuery3 = pd.concat([AppendedQuery2,AddedCustom3]).drop(columns=['FAB_IIO_Ratio'])\n",
    "    return AppendedQuery3\n",
    "\n",
    "\n",
    "def GetOverlapIndicator(row):\n",
    "    if (row[\"Scanners.Begin\"] is pd.NaT):\n",
    "        return \"No overlap\"\n",
    "    else:\n",
    "        if row[\"Scanners.Begin\"] < row[\"End\"] and row[\"Scanners.End\"] > row[\"Begin\"]:\n",
    "            if row[\"Begin\"] > row[\"Scanners.Begin\"] and row[\"End\"] < row[\"Scanners.End\"]:\n",
    "                return \"Useful overlap\"\n",
    "            else:\n",
    "                return \"Ignore overlap\"\n",
    "        else:\n",
    "            return \"No overlap\"\n",
    "\n",
    "\n",
    "def CheckForOverlaps(df):\n",
    "    Removed_Duplicates = df[\"Overlap\"].unique()\n",
    "    nb_of_rows = Removed_Duplicates.shape[0]\n",
    "    FirstValue = Removed_Duplicates[0]\n",
    "\n",
    "    if (nb_of_rows == 1) and (FirstValue == \"No overlap\"):\n",
    "        return True \n",
    "\n",
    "    return False\n",
    "\n",
    "def GetMinDate(row,col1, col2):\n",
    "    if (row[col1] is pd.NaT and row[col2] is not pd.NaT):\n",
    "        return row[col2]\n",
    "    elif (row[col1] is not pd.NaT and row[col2] is pd.NaT):\n",
    "        return row[col1]\n",
    "    else:\n",
    "        if row[col1] < row[col2]:\n",
    "            return row[col1]\n",
    "        else:\n",
    "            return row[col2]\n",
    "        \n",
    "def GetMaxDate(row,col1, col2):\n",
    "    if (row[col1] is pd.NaT and row[col2] is not pd.NaT):\n",
    "        return row[col2]\n",
    "    elif (row[col1] is not pd.NaT and row[col2] is pd.NaT):\n",
    "        return row[col1]\n",
    "    else:\n",
    "        if row[col1] < row[col2]:\n",
    "            return row[col2]\n",
    "        else:\n",
    "            return row[col1]\n",
    "\n",
    "\n",
    "def Final_Fab300_IIO_Reservations_clustered(df):\n",
    "    df = Final_Fab300_IIO_reservations_df.copy()\n",
    "    MergedQueries = df.merge(\n",
    "        LithoClusters, \n",
    "        how='left',\n",
    "        left_on = [\"Tool\"],\n",
    "        right_on = [\"ToolName\"]\n",
    "    )\n",
    "\n",
    "    print(\"MergedQueries\", MergedQueries.shape)\n",
    "    MergedQueries = MergedQueries.drop(columns=[\"ToolName\"])\n",
    "\n",
    "    SC_TR_filt = MergedQueries[\n",
    "        (MergedQueries[\"LithoCluster\"].notnull())\n",
    "        & (MergedQueries[\"LithoCluster\"] != \"\")\n",
    "    ]\n",
    "\n",
    "    SC_filt = SC_TR_filt[\n",
    "        SC_TR_filt[\"Tool\"].str.startswith(\"SC\")\n",
    "    ]\n",
    "    SC_filt['Tool'] = SC_filt['Tool'].str.replace('SC','LithoCluster_')\n",
    "    SC_to_Cluster = SC_filt.copy()\n",
    "    SC_index = SC_to_Cluster.copy()\n",
    "\n",
    "\n",
    "    index = range(0,len(SC_index))\n",
    "    SC_index[\"id\"] = index\n",
    "\n",
    "\n",
    "    TR_filt = SC_TR_filt[\n",
    "        SC_TR_filt[\"Tool\"].str.startswith(\"TR\")\n",
    "    ]\n",
    "    TR_index = TR_filt.copy()\n",
    "    index2 = range(0,len(TR_index))\n",
    "    TR_index[\"id\"] = index2\n",
    "\n",
    "\n",
    "    SC_index_sub = SC_index[\n",
    "        [\n",
    "            \"LithoCluster\", \"WBS\",\"Begin\", \"End\", \"id\"\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    TR_SC_Merge = TR_index.merge(\n",
    "        SC_index_sub, \n",
    "        how='left',\n",
    "        left_on = [\"LithoCluster\", \"WBS\"],\n",
    "        right_on = [\"LithoCluster\", \"WBS\"]\n",
    "    )\n",
    "    TR_SC_Expand = TR_SC_Merge.rename(\n",
    "    columns = {\n",
    "        \"Begin_y\":\"Scanners.Begin\",\n",
    "        \"End_y\":\"Scanners.End\",\n",
    "        \"Begin_x\":\"Begin\",\n",
    "        \"End_x\":\"End\",\n",
    "        \"id_y\":\"Scanners.id\",\n",
    "        \"id_x\":\"id\",\n",
    "        \"Cluster_x\": \"Cluster\",\n",
    "        'Description_x':\"Description\",\n",
    "        'Fab300_Duration_x':\"Fab300_Duration\",\n",
    "        'Fab300_User_id_x':\"Fab300_User_id\", \n",
    "        'FACILITY_x':\"FACILITY\", \n",
    "        'IIO_Duration_x':\"IIO_Duration\", \n",
    "        'IIO_User_id_x':\"IIO_User_id\",\n",
    "        'Modules_x':\"Modules\", \n",
    "        'Tool_x':\"Tool\"\n",
    "        \n",
    "    })\n",
    "\n",
    "    TR_SC_ovl_info = TR_SC_Expand.copy()\n",
    "    TR_SC_ovl_info[\"Overlap\"]  = TR_SC_ovl_info.apply(lambda row: GetOverlapIndicator(row),axis=1)\n",
    "\n",
    "    collated = pd.DataFrame(columns=[\n",
    "        'id', \n",
    "        'Independent'\n",
    "    ])\n",
    "\n",
    "\n",
    "    ids = TR_SC_ovl_info[\"id\"].unique()\n",
    "\n",
    "    for id in ids:\n",
    "        grp = TR_SC_ovl_info[TR_SC_ovl_info[\"id\"] == id]\n",
    "        grp =grp[[\"id\",\"Overlap\"]]\n",
    "        grp[\"Independent\"] = CheckForOverlaps(grp)\n",
    "        grp = grp.drop(columns=['Overlap']).drop_duplicates()\n",
    "        collated = pd.concat([collated,grp])\n",
    "    \n",
    "    collated = collated.drop_duplicates()\n",
    "    TR_pure = collated[collated[\"Independent\"] == True]\n",
    "    TR_pure = TR_pure[[\"id\",\"Independent\"]]\n",
    "\n",
    "    TR_merge  = TR_pure.merge(\n",
    "        TR_index, \n",
    "        how='inner',\n",
    "        left_on = [\"id\"],\n",
    "        right_on = [\"id\"]\n",
    "    )\n",
    "    TR_rmv = TR_merge.drop(columns=[\"id\",\"Independent\"])\n",
    "\n",
    "\n",
    "    SC_filt_2 = TR_SC_ovl_info[\n",
    "        (TR_SC_ovl_info[\"Overlap\"] != \"No overlap\")\n",
    "        & (TR_SC_ovl_info[\"Overlap\"] != \"Ignore overlap\")\n",
    "    ]\n",
    "    SC_TR_columns = SC_filt_2[[\"Cluster\", \"Begin\", \"End\", \"Scanners.id\"]]\n",
    "\n",
    "    SC_TR_columns2 = SC_TR_columns[\n",
    "        [\n",
    "            \"Scanners.id\",\n",
    "            \"Begin\", \n",
    "            \"End\", \n",
    "            \"Cluster\"\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    SC_pure = SC_index.merge(\n",
    "        SC_TR_columns2, \n",
    "        how='left',\n",
    "        left_on = [\"id\"],\n",
    "        right_on = [\"Scanners.id\"]\n",
    "    )\n",
    "    SC_pure = SC_pure.drop(columns=[\"Scanners.id\"])\n",
    "    SC_rename = SC_pure.rename(\n",
    "    columns={\n",
    "        \"Begin_y\": \"TR_Begin\", \n",
    "        \"End_y\": \"TR_End\", \n",
    "        \"Begin_x\": \"SC_Begin\", \n",
    "        \"End_x\": \"SC_End\", \n",
    "        \"Cluster_x\": \"Cluster\",\n",
    "        \"Cluster_y\": \"TR_Cluster\",\n",
    "        \n",
    "    })\n",
    "    SC_rename[\"Begin\"] = SC_rename.apply(lambda row: GetMinDate(row,\"SC_Begin\",\"TR_Begin\"),axis=1)\n",
    "    SC_rename[\"End\"] = SC_rename.apply(lambda row: GetMaxDate(row,\"SC_End\",\"TR_End\"),axis=1)\n",
    "    SC_final = SC_rename.drop(columns=[\"SC_Begin\", \"SC_End\", \"id\", \"TR_Begin\", \"TR_End\"])\n",
    "    NoClusters = MergedQueries[MergedQueries[\"LithoCluster\"].isna()]\n",
    "\n",
    "    Everything = pd.concat([NoClusters, TR_rmv])\n",
    "    Everything = pd.concat([Everything,SC_final])\n",
    "    Everything.drop_duplicates(inplace=True)\n",
    "    Everything = Everything.drop(columns=[\"LithoCluster\"])\n",
    "    index2 = range(0,len(Everything))\n",
    "    Everything[\"id\"] = index2\n",
    "    \n",
    "    return Everything\n",
    "\n",
    "\n",
    "def ReducedState(row):\n",
    "    if row[\"State\"] in [\"UP\",\"PARTLY_UP\",\"RESERVED\"]:\n",
    "        return \"UP\"\n",
    "    elif row[\"State\"] == \"SPC_TEST\":\n",
    "        return \"SPC\"\n",
    "    elif row[\"State\"] == \"OCAP\":\n",
    "        return \"OCAP\"\n",
    "    else:\n",
    "        return \"DOWN\"\n",
    "\n",
    "\n",
    "\n",
    "def Transform_states(df,fo_row_id):\n",
    "    df[\"ReducedState\"] = df.apply(lambda row: ReducedState(row),axis=1)\n",
    "    Removedcolumns1 = df.drop(columns=[\"State\", \"fo_row_id\"])\n",
    "    RnmStateClmn = Removedcolumns1.rename(\n",
    "    columns={\n",
    "        \"ReducedState\": \"State\"\n",
    "    })\n",
    "    Sortedrows = RnmStateClmn.sort_values(by=[\"EVENT_ROW_ID\"])\n",
    "    index =  range(0,len(Sortedrows))\n",
    "    Sortedrows[\"Index\"] = index\n",
    "    State_DOWN_Remove = Sortedrows.drop(columns=[\"Datim\", \"Index\", \"EVENT_ROW_ID\"]).rename(\n",
    "    columns={\n",
    "        \"State\": \"ReducedState_DOWN\"\n",
    "    })\n",
    "    Sortedrows[\"ReducedState_DOWN\"] = Sortedrows[\"State\"].shift(1)\n",
    "    Sortedrows[\"Repeated\"] = np.where(Sortedrows[\"ReducedState_DOWN\"] == Sortedrows[\"State\"],True,False)\n",
    "    Repeated_State = Sortedrows.copy()\n",
    "    NoRepeats = Repeated_State[Repeated_State[\"Repeated\"] == False]\n",
    "    RemoveTmpClmns = NoRepeats.drop(columns=[\"Index\", \"ReducedState_DOWN\", \"Repeated\"])\n",
    "\n",
    "    if RemoveTmpClmns.shape[0] == 0:\n",
    "        Sortedrows[\"fo_row_id\"] = fo_row_id\n",
    "        return Sortedrows[[\"fo_row_id\",\"Datim\",\"EVENT_ROW_ID\",\"State\"]].iloc[0:1]\n",
    "    else:\n",
    "        RemoveTmpClmns[\"fo_row_id\"] = fo_row_id\n",
    "        return RemoveTmpClmns\n",
    "\n",
    "\n",
    "def RemoveRepeats(df):\n",
    "    RC = df.drop(columns=[\"ENT_NAME\",\"FACILITY\"])\n",
    "    sorted_rows = RC.sort_values([\"EVENT_ROW_ID\"])\n",
    "    index = range(0,len(sorted_rows))\n",
    "    sorted_rows[\"Index\"] = index\n",
    "    State_UP_Add_Index = sorted_rows.copy()\n",
    "    State_UP_Add_Index[\"Datim_UP\"] = State_UP_Add_Index[\"Datim\"].shift(-1)\n",
    "    State_UP_Add_Index[\"Datim_UP\"] = pd.to_datetime(State_UP_Add_Index[\"Datim_UP\"],format=\"%m/%d/%Y %H:%M\")\n",
    "    State_UP_Add_Index[\"Datim\"] = pd.to_datetime(State_UP_Add_Index[\"Datim\"],format=\"%m/%d/%Y %H:%M\")\n",
    "    ReplacedValue = State_UP_Add_Index.copy()\n",
    "    ReplacedValue[\"Datim_UP\"] = ReplacedValue[\"Datim_UP\"].fillna(datetime.now())\n",
    "    ReplacedValue[\"Duration_hrs\"] = (ReplacedValue[\"Datim_UP\"] - ReplacedValue[\"Datim\"])/np.timedelta64(1, 'h')\n",
    "    ReplacedValue[\"Duration_mins\"] = (ReplacedValue[\"Datim_UP\"] - ReplacedValue[\"Datim\"])/np.timedelta64(1, 'm')\n",
    "    FilteredRows2 = ReplacedValue[ReplacedValue[\"Duration_mins\"] > 10]\n",
    "\n",
    "    RemovedColumns1 = FilteredRows2.drop(columns=[\"Index\", \"Datim_UP\", \"Duration_hrs\",\"Duration_mins\"])\n",
    "    Sortedrows1 = RemovedColumns1.sort_values([\"EVENT_ROW_ID\"])\n",
    "    With_DOWN_Expanded = Sortedrows1.copy()\n",
    "    With_DOWN_Expanded[\"State_DOWN\"] = With_DOWN_Expanded[\"State\"].shift(1)\n",
    "    With_DOWN_Expanded[\"Repeated\"] = np.where(With_DOWN_Expanded['State']==With_DOWN_Expanded['State_DOWN'], True, False)\n",
    "    NoRepeats = With_DOWN_Expanded[With_DOWN_Expanded[\"Repeated\"] == False]\n",
    "    RemovedColumns2 = NoRepeats.drop(columns=[\"State_DOWN\", \"Repeated\"])\n",
    "    return RemovedColumns2[[\n",
    "        \"EVENT_ROW_ID\", \n",
    "         \"Datim\", \n",
    "         \"State\"\n",
    "    ]]\n",
    "\n",
    "\n",
    "def GetClusterStates(df):\n",
    "    SC = df[df[\"ENT_NAME\"].str.startswith(\"SC\")]\n",
    "    SC_rem_clmns = SC.drop(columns=[\"FACILITY\", \"ENT_NAME\", \"LithoCluster\"])\n",
    "    SC_renamed = SC_rem_clmns.rename(columns={\n",
    "        \"State\": \"State_SC\"\n",
    "    })\n",
    "\n",
    "\n",
    "    TR = df[df[\"ENT_NAME\"].str.startswith(\"TR\")]\n",
    "    TR_rem_clmns = TR.drop(columns=[\"FACILITY\", \"ENT_NAME\", \"LithoCluster\"])\n",
    "    TR_renamed = TR_rem_clmns.rename(columns={\n",
    "        \"State\": \"State_TR\"\n",
    "    })\n",
    "    TR_renamed\n",
    "\n",
    "    CombinedQueries = pd.concat([SC_renamed,TR_renamed])\n",
    "\n",
    "    SortedRows = CombinedQueries.sort_values([\"EVENT_ROW_ID\"])\n",
    "    FilledDown = SortedRows.copy()\n",
    "    FilledDown[\"State_SC\"] = FilledDown[\"State_SC\"].fillna(method='ffill')\n",
    "    FilledDown[\"State_TR\"] = FilledDown[\"State_TR\"].fillna(method='ffill')\n",
    "    FilledDown[\"State_SC\"] = FilledDown[\"State_SC\"].fillna('DOWN')\n",
    "    FilledDown[\"State_TR\"] = FilledDown[\"State_TR\"].fillna('DOWN')\n",
    "    AddedCustom1 = FilledDown.copy()\n",
    "    AddedCustom1[\"State\"] = np.where((AddedCustom1[\"State_SC\"] == \"UP\") & (AddedCustom1[\"State_TR\"] == \"UP\"),\"UP\",\"DOWN\")\n",
    "\n",
    "    return AddedCustom1\n",
    "\n",
    "def Tools_states_material_suppliers():\n",
    "    Expanded_Tools_parents = tool_states_result.merge(\n",
    "        Tools_Parents, \n",
    "        how='inner',\n",
    "        left_on = [\"fo_row_id\"],\n",
    "        right_on = [\"ROW_ID\"],\n",
    "        indicator=True\n",
    "    )\n",
    "\n",
    "    \n",
    "    Filtered_rows = Expanded_Tools_parents[\n",
    "        (Expanded_Tools_parents[\"ENT_NAME\"].notnull())\n",
    "        & (Expanded_Tools_parents[\"ENT_NAME\"] != \"\")\n",
    "    ]\n",
    "    Filtered_rows[\"State\"] = Filtered_rows.apply(lambda row: replaceStateValueSPC(row),axis=1)\n",
    "    Filtered_rows[\"State\"] = Filtered_rows.apply(lambda row: replaceStateValueOCAP(row),axis=1)\n",
    "\n",
    "    Removed_Columns = Filtered_rows.drop(columns=[\n",
    "        \"Area\", \n",
    "        \"fo_row_id\",\n",
    "        \"_merge\",\n",
    "        \"ROW_ID\"\n",
    "    ])\n",
    "    #Removed_Columns = Removed_Columns[Removed_Columns[\"ENT_NAME\"].isin(['TR3400'])]\n",
    "    GroupedRows = pd.DataFrame(columns=[\n",
    "        \"FACILITY\",\n",
    "        \"ENT_NAME\",\n",
    "        \"EVENT_ROW_ID\", \n",
    "        \"Datim\", \n",
    "        \"State\"\n",
    "    ])\n",
    "    facilities = Removed_Columns[\"FACILITY\"].unique()\n",
    "    print(facilities)\n",
    "    for facility in facilities:\n",
    "        facilitydata = Removed_Columns[Removed_Columns[\"FACILITY\"] == facility]\n",
    "        ent_names = facilitydata[\"ENT_NAME\"].unique()\n",
    "        \n",
    "        for ent_name in ent_names:\n",
    "            data = facilitydata[facilitydata[\"ENT_NAME\"] == ent_name]\n",
    "            NoRepeats = RemoveRepeats(data)\n",
    "            NoRepeats[\"FACILITY\"] = facility\n",
    "            NoRepeats[\"ENT_NAME\"] = ent_name\n",
    "            \n",
    "            GroupedRows = pd.concat([GroupedRows,NoRepeats])\n",
    "    \n",
    "    LithoClusters[\"LithoCluster\"] = LithoClusters[\"LithoCluster\"].map(str)\n",
    "    MergedQueries = pd.merge(\n",
    "            GroupedRows, \n",
    "            LithoClusters, \n",
    "            left_on=[\"ENT_NAME\"], \n",
    "            right_on=[\"ToolName\"], \n",
    "            how=\"left\",\n",
    "            suffixes=[\"\",\"_y\"]\n",
    "        )\n",
    "    \n",
    "    MergedQueries = MergedQueries.drop(columns = ['ToolName'])\n",
    "\n",
    "    ClusterStates = MergedQueries[(MergedQueries[\"LithoCluster\"].notnull()) & (MergedQueries[\"LithoCluster\"] != \"\")]\n",
    "\n",
    "\n",
    "\n",
    "    GrClusterStates = pd.DataFrame(columns=[\n",
    "        'EVENT_ROW_ID', \n",
    "        'Datim', \n",
    "        'State_SC', \n",
    "        'State_TR', \n",
    "        'State',\n",
    "        'FACILITY',\n",
    "        'LithoCluster'\n",
    "        \n",
    "    ])\n",
    "    \n",
    "    facilities = ClusterStates[\"FACILITY\"].unique()\n",
    "\n",
    "    for facility in facilities:\n",
    "        facilitydata = ClusterStates[ClusterStates[\"FACILITY\"] == facility]\n",
    "        Litho_Clusters = facilitydata[\"LithoCluster\"].unique()\n",
    "        \n",
    "        for LithoCluster in Litho_Clusters:\n",
    "            data = facilitydata[facilitydata[\"LithoCluster\"] == LithoCluster]\n",
    "            ClusterStatesData = GetClusterStates(data)\n",
    "            ClusterStatesData[\"FACILITY\"] = facility\n",
    "            ClusterStatesData[\"LithoCluster\"] = LithoCluster\n",
    "            \n",
    "            GrClusterStates = pd.concat([GrClusterStates,ClusterStatesData])\n",
    "            print(facility,LithoCluster)\n",
    "            \n",
    "    \n",
    "    GrExpand = GrClusterStates.copy()\n",
    "    GrExpand[\"LithoCluster\"] = GrExpand[\"LithoCluster\"].map(str)\n",
    "\n",
    "    GrExpand[\"LithoCluster\"] = [\"LithoCluster_\" + str(x) for x in GrExpand[\"LithoCluster\"]]\n",
    "\n",
    "    GrReplace = GrExpand.copy()\n",
    "    GrRename = GrReplace.rename(columns={\n",
    "        \"LithoCluster\":\"ENT_NAME\"\n",
    "    })\n",
    "    TrStates = GrRename.copy()\n",
    "    TrStates[\"ENT_NAME\"] = TrStates[\"ENT_NAME\"].str.replace(\"LithoCluster_\",\"TR\")\n",
    "    TrStates[\"State\"] = TrStates[\"State_TR\"]\n",
    "    NonClusters = MergedQueries[MergedQueries[\"LithoCluster\"].isnull()]\n",
    "    NonClustersRC = NonClusters.drop(columns=[\"LithoCluster\"])\n",
    "    Combined = pd.concat([NonClustersRC, GrRename, TrStates])\n",
    "    Combined = Combined.rename(columns={\"ENT_NAME\": \"Tool\"})\n",
    "    return Combined\n",
    "        \n",
    "\n",
    "\n",
    "def replaceStateValueSPC(row):\n",
    "    if (\"ILIME\" in row[\"Area\"]) or (row[\"ENT_NAME\"] in AllTracks):\n",
    "        return row[\"State\"].replace(\"SPC\",\"UP\")\n",
    "    return  row[\"State\"].replace(\"SPC\",\"DOWN\")\n",
    "\n",
    "def replaceStateValueOCAP(row):\n",
    "    if (row[\"ENT_NAME\"] in AllTracks):\n",
    "        return row[\"State\"].replace(\"OCAP\",\"UP\")\n",
    "    return  row[\"State\"].replace(\"OCAP\",\"DOWN\")\n",
    "\n",
    "def OverlapStatusWithReservations(df,Facility,Tool):\n",
    "    RC = df.drop(columns=[\"Facility\", \"Tool\"])\n",
    "    SortedRows = RC.sort_values(by=[\"Datim\",\"BeginEndFlag\"],ascending=[True,False])\n",
    "    FilledDown = SortedRows.apply(lambda x: x.fillna(method='ffill'))\n",
    "    FilteredRows = FilledDown[\n",
    "        (FilledDown[\"BeginEndFlag\"] != \"End\")\n",
    "        & (FilledDown[\"BeginEndFlag\"].notnull())\n",
    "    ]\n",
    "    if FilteredRows.shape[0] == 0:\n",
    "        blank = pd.DataFrame(columns=df.columns)\n",
    "        blank = blank.append({'Facility':Facility, 'Tool':Tool},ignore_index=True)\n",
    "        \n",
    "        return blank\n",
    "    return FilteredRows\n",
    "\n",
    "def Reservations_Tools_States_Details(Tools_states_material_suppliers_df,Final_Fab300_IIO_Reservations_clustered_df):\n",
    "    States = Tools_states_material_suppliers_df.drop(columns=[\"EVENT_ROW_ID\"])\n",
    "    States = States.rename(columns={\n",
    "        \"FACILITY\": \"Facility\"\n",
    "    })\n",
    "    Reservations = Final_Fab300_IIO_Reservations_clustered_df.copy()\n",
    "    Reservations[\"Duration\"] = (Reservations[\"End\"] - Reservations[\"Begin\"])/np.timedelta64(1, 'h')\n",
    "    Removed_Other_Columns = Reservations[[\"FACILITY\", \"Tool\", \"Begin\", \"End\", \"Duration\", \"id\"]]\n",
    "    Unpivoted_Columns = pd.melt(Removed_Other_Columns, id_vars=[\"FACILITY\", \"Tool\", \"id\", \"Duration\"], \n",
    "                    value_vars=[\"Begin\", \"End\"])\n",
    "    Unpivoted_Columns = Unpivoted_Columns.rename(columns={\n",
    "        \"variable\": \"BeginEndFlag\",\n",
    "        \"value\": \"Datim\",\n",
    "        \"FACILITY\": \"Facility\",\n",
    "    })\n",
    "    Appended_Query = pd.concat([States,Unpivoted_Columns])\n",
    "\n",
    "    GroupedRows = pd.DataFrame(\n",
    "        columns = [\n",
    "            'Facility', \n",
    "            'Tool', \n",
    "            'Datim', \n",
    "            'State', \n",
    "            'State_SC', \n",
    "            'State_TR', \n",
    "            'id',\n",
    "            'Duration', \n",
    "            'BeginEndFlag'\n",
    "        ]\n",
    "    )\n",
    "    i = 0\n",
    "    facilities = Appended_Query[\"Facility\"].unique()\n",
    "\n",
    "    for facility in facilities:\n",
    "        facilityData = Appended_Query[Appended_Query[\"Facility\"] == facility]\n",
    "\n",
    "        tools = facilityData[\"Tool\"].unique()\n",
    "\n",
    "        for tool in tools:\n",
    "            i = i + 1\n",
    "            tooldata = facilityData[facilityData[\"Tool\"] == tool]\n",
    "            df = OverlapStatusWithReservations(tooldata,facility,tool)\n",
    "            df[\"Facility\"] = facility\n",
    "            df[\"Tool\"] = tool\n",
    "            GroupedRows = pd.concat([GroupedRows,df])\n",
    "\n",
    "    ExpandedRows = GroupedRows.drop(columns=[\"BeginEndFlag\"])\n",
    "    ReplacedValue = ExpandedRows.copy()\n",
    "    ReplacedValue[\"State\"] = ReplacedValue[\"State\"].fillna(\"UP\")\n",
    "    ReplacedValue[\"State\"] = ReplacedValue[\"State\"].apply(lambda x: \"UP\" if len(x)==0 else x)\n",
    "    return ReplacedValue\n",
    "\n",
    "def CountBillableTime(df):\n",
    "    x = df.sort_values(by=[\"Datim\"])\n",
    "    FirstRow = x.iloc[0]\n",
    "    LastRow = x.iloc[-1]\n",
    "    FullDuration = FirstRow[\"Duration\"]\n",
    "    Duration = (LastRow[\"Datim\"] - FirstRow[\"Datim\"])/np.timedelta64(1, 'h')\n",
    "    NumberOfRows = x.shape[0]\n",
    "    if NumberOfRows == 1:\n",
    "        if FirstRow[\"State\"] == \"UP\":\n",
    "            GoodBadDuration = FullDuration\n",
    "        else:\n",
    "            GoodBadDuration = 0\n",
    "    else:\n",
    "        if NumberOfRows == 2:\n",
    "            if FirstRow[\"State\"] == \"UP\":\n",
    "                GoodBadDuration = 0\n",
    "            else:\n",
    "                if Duration / FullDuration < 0.5:\n",
    "                    GoodBadDuration = FullDuration - Duration\n",
    "                else:\n",
    "                    GoodBadDuration = 0\n",
    "        else:\n",
    "            GoodBadDuration = 0\n",
    "    return GoodBadDuration\n",
    "\n",
    "def Final_Reservations_Overlapped_with_Tool_States(Reservations_Tools_States_Details_df,Final_Fab300_IIO_Reservations_clustered_df):\n",
    "    IDs = Reservations_Tools_States_Details_df[\"id\"].dropna().unique()\n",
    "    billables = pd.DataFrame(columns=[\"id\",\"BillableDuration\"])\n",
    "    for id in IDs:\n",
    "        df = Reservations_Tools_States_Details_df[Reservations_Tools_States_Details_df[\"id\"] == id]\n",
    "        billable = CountBillableTime(df)\n",
    "        billables = billables.append({'id':id, 'BillableDuration':billable},ignore_index=True)\n",
    "\n",
    "    billables[\"BillableDuration\"] = round(billables[\"BillableDuration\"] * 4,0) / 4\n",
    "    Merge = pd.merge(\n",
    "            Final_Fab300_IIO_Reservations_clustered_df, \n",
    "            billables, \n",
    "            left_on=[\"id\"], \n",
    "            right_on=[\"id\"], \n",
    "            how=\"left\",\n",
    "            suffixes=[\"\",\"_y\"]\n",
    "        )\n",
    "    return Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "aef870f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MergedQueries (2835, 14)\n",
      "['PLINE300' 'PLINE200']\n",
      "PLINE300 1950\n",
      "PLINE300 3300\n",
      "PLINE300 2000\n",
      "PLINE300 1000\n",
      "PLINE300 3400\n",
      "(2651, 15)\n"
     ]
    }
   ],
   "source": [
    "#Start of execution\n",
    "folder = \"C:\\\\Users\\\\fpicaso\\\\Repos\\\\PMOPS\\\\20230130\\\\\"\n",
    "Fab300_Raw = pd.read_csv(folder + \"Fab300 Raw Reservations.csv\")\n",
    "\n",
    "#format the date\n",
    "Fab300_Raw[\"DATE_TIME_STAMP\"] = pd.to_datetime(Fab300_Raw[\"DATE_TIME_STAMP\"],format=\"%d/%m/%Y %H:%M\")\n",
    "Tools_with_reservations = processFab300RawReservations(Fab300_Raw)\n",
    "\n",
    "Tools_Parents = pd.read_csv(folder + \"Tools_Parents.csv\")\n",
    "Tools_Parents[\"CSIM_TIMESTAMP\"] = pd.to_datetime(Tools_Parents[\"CSIM_TIMESTAMP\"])\n",
    "\n",
    "df_FAB300_with_tool_names = FAB300_with_tool_names(Tools_with_reservations,Tools_Parents)\n",
    "#df_FAB300_with_tool_names = pd.read_csv(folder + \"7. Fab300 with tool names.csv\")\n",
    "            \n",
    "\n",
    "IIO_raw = pd.read_csv(folder + \"4. IIO_raw_reservations .csv\")\n",
    "\n",
    "#format the date\n",
    "IIO_raw[\"Begin\"] = pd.to_datetime(IIO_raw[\"Begin\"],format=\"%Y-%m-%d %H:%M\")\n",
    "IIO_raw[\"End\"] = pd.to_datetime(IIO_raw[\"End\"],format=\"%Y-%m-%d %H:%M\")\n",
    "    \n",
    "df_IIO_without_modules = IIO_without_modules(IIO_raw)\n",
    "\n",
    "#df_IIO_without_modules = pd.read_csv(folder + \"8. IIO_without_modules.csv\")\n",
    "\n",
    "\n",
    "#Fab300_IIO_overlaps_ids\n",
    "Source_fab = df_FAB300_with_tool_names.copy()\n",
    "RC_fab = Source_fab.drop(columns=[\"ResTk\", \"User_id\"])\n",
    "UnPivot_fab = pd.melt(RC_fab, id_vars=[\"WBS\", \"FACILITY\", \"Tool\", \"Fab300_Res_id\"], \n",
    "                value_vars=[\"Begin\", \"End\"])\n",
    "UnPivot_fab = UnPivot_fab.rename(\n",
    "columns={\n",
    "    \"variable\": \"FAB300_BeginEnd\",\n",
    "    \"value\": \"DateTime\"\n",
    "})\n",
    "UnPivot_fab[\"FAB_IIO_Flag\"] = \"FAB\"\n",
    "\n",
    "Source_iio = df_IIO_without_modules.copy()\n",
    "RC_iio = Source_iio.drop(columns=[\"Modules\", \"User_id\", \"Description\"])\n",
    "UnPivot_iio = pd.melt(RC_iio, id_vars=[\"WBS\", \"FACILITY\", \"Tool\", \"IIO_Res_id\"], \n",
    "                value_vars=[\"Begin\", \"End\"])\n",
    "\n",
    "UnPivot_iio = UnPivot_iio.rename(\n",
    "columns={\n",
    "    \"variable\": \"IIO_BeginEnd\",\n",
    "    \"value\": \"DateTime\"\n",
    "})\n",
    "UnPivot_iio[\"FAB_IIO_Flag\"] = \"IIO\"\n",
    "\n",
    "fab_iio_together  = pd.concat([UnPivot_fab, UnPivot_iio], ignore_index=True)\n",
    "\n",
    "columns = [\n",
    "    'FAB_IIO_Flag',\n",
    "    'Fab300_Res_id', \n",
    "    'IIO_Res_id', \n",
    "    'Cluster', \n",
    "    'Fab300_Begin', \n",
    "    'Fab300_End',\n",
    "    'FACILITY', \n",
    "    'ResTk', \n",
    "    'Tool', \n",
    "    'Fab300_User_id', \n",
    "    'WBS', \n",
    "    'IIO_Begin',\n",
    "    'Description', \n",
    "    'IIO_End'\n",
    "]\n",
    "\n",
    "Fab300_IIO_overlaps_ids = pd.DataFrame(columns = columns)\n",
    "\n",
    "WBSs = fab_iio_together[\"WBS\"].unique()\n",
    "\n",
    "for wbs in WBSs:\n",
    "    wbsdata = fab_iio_together[\n",
    "        (fab_iio_together[\"WBS\"] == wbs)\n",
    "    ]\n",
    "    facilities = wbsdata[\"FACILITY\"].unique()\n",
    "    for facility in facilities:\n",
    "        facilityData = wbsdata[\n",
    "            (wbsdata[\"FACILITY\"] == facility)\n",
    "        ]\n",
    "        tools = facilityData[\"Tool\"].unique()\n",
    "        for tool in tools:\n",
    "            wbstooldata = facilityData[\n",
    "                (wbsdata[\"Tool\"] == tool)\n",
    "            ]\n",
    "            df = Fab300_iio_merger(wbstooldata)\n",
    "            Fab300_IIO_overlaps_ids = pd.concat([Fab300_IIO_overlaps_ids,df])\n",
    "            \n",
    "\n",
    "\n",
    "#Final_Fab300_IIO_reservations\n",
    "\n",
    "Final_Fab300_IIO_reservations_df = Final_Fab300_IIO_reservations(df_IIO_without_modules,df_FAB300_with_tool_names,Fab300_IIO_overlaps_ids)\n",
    "\n",
    "\n",
    "#Step 14\n",
    "Final_Fab300_IIO_reservations_df[\"Begin\"] = pd.to_datetime(Final_Fab300_IIO_reservations_df[\"Begin\"],format=\"%Y-%m-%d %H:%M\")\n",
    "Final_Fab300_IIO_reservations_df[\"End\"] = pd.to_datetime(Final_Fab300_IIO_reservations_df[\"End\"],format=\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "\n",
    "LithoClusters = pd.read_csv(folder + \"13. LithoClusters.csv\")\n",
    "\n",
    "Final_Fab300_IIO_Reservations_clustered_df = Final_Fab300_IIO_Reservations_clustered(Final_Fab300_IIO_reservations_df)\n",
    "\n",
    "#Tool States and Transform States\n",
    "\n",
    "tool_states = pd.read_csv(folder + \"Tool_States.csv\")\n",
    "tool_states[\"Datim\"] = pd.to_datetime(tool_states[\"Datim\"],format=\"%d/%m/%Y %H:%M\")\n",
    "\n",
    "fo_row_ids = tool_states[\"fo_row_id\"].unique()\n",
    "\n",
    "tool_states_result = pd.DataFrame(\n",
    "columns=[\n",
    "    \"Datim\",\n",
    "    \"EVENT_ROW_ID\",\n",
    "    \"State\"\n",
    "])\n",
    "\n",
    "for fo_row_id in fo_row_ids:\n",
    "    df = tool_states[tool_states[\"fo_row_id\"] == fo_row_id]\n",
    "    tool_states_df = Transform_states(df,fo_row_id)\n",
    "    tool_states_result = pd.concat([tool_states_result,tool_states_df])\n",
    "\n",
    "\n",
    "\n",
    "#17. Tools_states_material_suppliers\n",
    "AllTracks = [\"TR1000\", \"TR1950\", \"TR1970\", \"TR2000\", \"TR3300\", \"TR3400\", \"TRDSA\", \"TRMTM\", \"TRDUOS\"]\n",
    "\n",
    "Tools_states_material_suppliers_df = Tools_states_material_suppliers()\n",
    "\n",
    "#18. Reservations_Tools_States_Details\n",
    "\n",
    "Reservations_Tools_States_Details_df = Reservations_Tools_States_Details(Tools_states_material_suppliers_df,Final_Fab300_IIO_Reservations_clustered_df)\n",
    "\n",
    "#19. Final_Reservations_Overlapped_with_Tool_States\n",
    "Final_Reservations_Overlapped_with_Tool_States_df = Final_Reservations_Overlapped_with_Tool_States(Reservations_Tools_States_Details_df,Final_Fab300_IIO_Reservations_clustered_df)\n",
    "print(Final_Reservations_Overlapped_with_Tool_States_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "782ec8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "billables[\"BillableDuration\"] = billables[\"BillableDuration\"].astype(float) * 4 / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "41e67477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FACILITY</th>\n",
       "      <th>Tool</th>\n",
       "      <th>WBS</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Description</th>\n",
       "      <th>Begin</th>\n",
       "      <th>End</th>\n",
       "      <th>IIO_User_id</th>\n",
       "      <th>Fab300_User_id</th>\n",
       "      <th>Modules</th>\n",
       "      <th>Fab300_Duration</th>\n",
       "      <th>IIO_Duration</th>\n",
       "      <th>TR_Cluster</th>\n",
       "      <th>id</th>\n",
       "      <th>BillableDuration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PLINE200</td>\n",
       "      <td>300MM_DICER</td>\n",
       "      <td>32299/02016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Prio lot</td>\n",
       "      <td>2023-01-16 08:25:00</td>\n",
       "      <td>2023-01-16 17:00:00</td>\n",
       "      <td>schoofs</td>\n",
       "      <td>schoofs</td>\n",
       "      <td></td>\n",
       "      <td>8.566667</td>\n",
       "      <td>7.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PLINE200</td>\n",
       "      <td>300MM_DICER</td>\n",
       "      <td>32299/02016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>For Alginment issue lot process</td>\n",
       "      <td>2023-01-19 09:30:00</td>\n",
       "      <td>2023-01-19 16:48:00</td>\n",
       "      <td>hiro13</td>\n",
       "      <td>hiro13</td>\n",
       "      <td></td>\n",
       "      <td>7.266667</td>\n",
       "      <td>6.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PLINE200</td>\n",
       "      <td>300MM_DICER</td>\n",
       "      <td>36671/01224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>W2W bonding ET overlay backup</td>\n",
       "      <td>2023-01-13 09:30:00</td>\n",
       "      <td>2023-01-13 13:27:00</td>\n",
       "      <td>ameghn57</td>\n",
       "      <td>ameghn57</td>\n",
       "      <td></td>\n",
       "      <td>2.833333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PLINE200</td>\n",
       "      <td>ACT12_MTM</td>\n",
       "      <td>37004/37141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DFR</td>\n",
       "      <td>2023-01-12 09:00:00</td>\n",
       "      <td>2023-01-12 13:30:00</td>\n",
       "      <td>eswara57</td>\n",
       "      <td>cooman32</td>\n",
       "      <td></td>\n",
       "      <td>1.516667</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PLINE200</td>\n",
       "      <td>ACT12_MTM</td>\n",
       "      <td>37004/37141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>DFR</td>\n",
       "      <td>2023-01-20 09:00:00</td>\n",
       "      <td>2023-01-20 13:49:00</td>\n",
       "      <td>eswara57</td>\n",
       "      <td>haever53</td>\n",
       "      <td></td>\n",
       "      <td>3.816667</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2646</th>\n",
       "      <td>PLINE300</td>\n",
       "      <td>LithoCluster_3400</td>\n",
       "      <td>98601/01971</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dev</td>\n",
       "      <td>2023-01-17 10:00:00</td>\n",
       "      <td>2023-01-17 11:00:00</td>\n",
       "      <td>thiam14</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2646</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2647</th>\n",
       "      <td>PLINE300</td>\n",
       "      <td>LithoCluster_3400</td>\n",
       "      <td>31199/01514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JSR</td>\n",
       "      <td>2023-01-21 19:00:00</td>\n",
       "      <td>2023-01-21 20:00:00</td>\n",
       "      <td>furuka28</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2647</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648</th>\n",
       "      <td>PLINE300</td>\n",
       "      <td>LithoCluster_3400</td>\n",
       "      <td>31199/00653</td>\n",
       "      <td>NaN</td>\n",
       "      <td>source calibration by ASML</td>\n",
       "      <td>2023-01-10 23:00:00</td>\n",
       "      <td>2023-01-11 00:00:00</td>\n",
       "      <td>versluij</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2648</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2649</th>\n",
       "      <td>PLINE300</td>\n",
       "      <td>LithoCluster_3400</td>\n",
       "      <td>31199/01413</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-16 12:51:00</td>\n",
       "      <td>2023-01-16 13:20:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vdries51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2649</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2650</th>\n",
       "      <td>PLINE300</td>\n",
       "      <td>LithoCluster_3400</td>\n",
       "      <td>31199/31811</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-20 06:25:00</td>\n",
       "      <td>2023-01-20 07:02:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sterckxg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2650</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2651 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      FACILITY               Tool          WBS  Cluster  \\\n",
       "0     PLINE200        300MM_DICER  32299/02016      0.0   \n",
       "1     PLINE200        300MM_DICER  32299/02016      1.0   \n",
       "2     PLINE200        300MM_DICER  36671/01224      0.0   \n",
       "3     PLINE200          ACT12_MTM  37004/37141      0.0   \n",
       "4     PLINE200          ACT12_MTM  37004/37141      1.0   \n",
       "...        ...                ...          ...      ...   \n",
       "2646  PLINE300  LithoCluster_3400  98601/01971      NaN   \n",
       "2647  PLINE300  LithoCluster_3400  31199/01514      NaN   \n",
       "2648  PLINE300  LithoCluster_3400  31199/00653      NaN   \n",
       "2649  PLINE300  LithoCluster_3400  31199/01413      NaN   \n",
       "2650  PLINE300  LithoCluster_3400  31199/31811      NaN   \n",
       "\n",
       "                          Description               Begin                 End  \\\n",
       "0                            Prio lot 2023-01-16 08:25:00 2023-01-16 17:00:00   \n",
       "1     For Alginment issue lot process 2023-01-19 09:30:00 2023-01-19 16:48:00   \n",
       "2       W2W bonding ET overlay backup 2023-01-13 09:30:00 2023-01-13 13:27:00   \n",
       "3                                 DFR 2023-01-12 09:00:00 2023-01-12 13:30:00   \n",
       "4                                 DFR 2023-01-20 09:00:00 2023-01-20 13:49:00   \n",
       "...                               ...                 ...                 ...   \n",
       "2646                              dev 2023-01-17 10:00:00 2023-01-17 11:00:00   \n",
       "2647                              JSR 2023-01-21 19:00:00 2023-01-21 20:00:00   \n",
       "2648       source calibration by ASML 2023-01-10 23:00:00 2023-01-11 00:00:00   \n",
       "2649                              NaN 2023-01-16 12:51:00 2023-01-16 13:20:00   \n",
       "2650                              NaN 2023-01-20 06:25:00 2023-01-20 07:02:00   \n",
       "\n",
       "     IIO_User_id Fab300_User_id Modules  Fab300_Duration  IIO_Duration  \\\n",
       "0        schoofs        schoofs                 8.566667           7.5   \n",
       "1         hiro13         hiro13                 7.266667           6.5   \n",
       "2       ameghn57       ameghn57                 2.833333           3.0   \n",
       "3       eswara57       cooman32                 1.516667           4.5   \n",
       "4       eswara57       haever53                 3.816667           4.0   \n",
       "...          ...            ...     ...              ...           ...   \n",
       "2646     thiam14            NaN                      NaN           1.0   \n",
       "2647    furuka28            NaN                      NaN           1.0   \n",
       "2648    versluij            NaN                      NaN           1.0   \n",
       "2649         NaN       vdries51     NaN         0.483333           NaN   \n",
       "2650         NaN       sterckxg     NaN         0.616667           NaN   \n",
       "\n",
       "      TR_Cluster    id  BillableDuration  \n",
       "0            NaN     0              8.50  \n",
       "1            NaN     1              7.25  \n",
       "2            NaN     2              4.00  \n",
       "3            NaN     3              4.50  \n",
       "4            NaN     4              4.75  \n",
       "...          ...   ...               ...  \n",
       "2646         NaN  2646              1.00  \n",
       "2647         NaN  2647              1.00  \n",
       "2648         NaN  2648              1.00  \n",
       "2649         NaN  2649              0.50  \n",
       "2650         NaN  2650              0.50  \n",
       "\n",
       "[2651 rows x 15 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3e5b46e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FACILITY</th>\n",
       "      <th>Tool</th>\n",
       "      <th>WBS</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Description</th>\n",
       "      <th>Begin</th>\n",
       "      <th>End</th>\n",
       "      <th>IIO_User_id</th>\n",
       "      <th>Fab300_User_id</th>\n",
       "      <th>Modules</th>\n",
       "      <th>Fab300_Duration</th>\n",
       "      <th>IIO_Duration</th>\n",
       "      <th>TR_Cluster</th>\n",
       "      <th>id</th>\n",
       "      <th>BillableDuration</th>\n",
       "      <th>BillableDuration1</th>\n",
       "      <th>BillableDuration2</th>\n",
       "      <th>BillableDuration3</th>\n",
       "      <th>BillableDuration4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PLINE200</td>\n",
       "      <td>300MM_DICER</td>\n",
       "      <td>32299/02016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Prio lot</td>\n",
       "      <td>2023-01-16 08:25:00</td>\n",
       "      <td>2023-01-16 17:00:00</td>\n",
       "      <td>schoofs</td>\n",
       "      <td>schoofs</td>\n",
       "      <td></td>\n",
       "      <td>8.566667</td>\n",
       "      <td>7.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34.333333</td>\n",
       "      <td>34.0</td>\n",
       "      <td>8.50</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PLINE200</td>\n",
       "      <td>300MM_DICER</td>\n",
       "      <td>32299/02016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>For Alginment issue lot process</td>\n",
       "      <td>2023-01-19 09:30:00</td>\n",
       "      <td>2023-01-19 16:48:00</td>\n",
       "      <td>hiro13</td>\n",
       "      <td>hiro13</td>\n",
       "      <td></td>\n",
       "      <td>7.266667</td>\n",
       "      <td>6.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.200000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PLINE200</td>\n",
       "      <td>300MM_DICER</td>\n",
       "      <td>36671/01224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>W2W bonding ET overlay backup</td>\n",
       "      <td>2023-01-13 09:30:00</td>\n",
       "      <td>2023-01-13 13:27:00</td>\n",
       "      <td>ameghn57</td>\n",
       "      <td>ameghn57</td>\n",
       "      <td></td>\n",
       "      <td>2.833333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PLINE200</td>\n",
       "      <td>ACT12_MTM</td>\n",
       "      <td>37004/37141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DFR</td>\n",
       "      <td>2023-01-12 09:00:00</td>\n",
       "      <td>2023-01-12 13:30:00</td>\n",
       "      <td>eswara57</td>\n",
       "      <td>cooman32</td>\n",
       "      <td></td>\n",
       "      <td>1.516667</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.50</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PLINE200</td>\n",
       "      <td>ACT12_MTM</td>\n",
       "      <td>37004/37141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>DFR</td>\n",
       "      <td>2023-01-20 09:00:00</td>\n",
       "      <td>2023-01-20 13:49:00</td>\n",
       "      <td>eswara57</td>\n",
       "      <td>haever53</td>\n",
       "      <td></td>\n",
       "      <td>3.816667</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.266667</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.75</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2646</th>\n",
       "      <td>PLINE300</td>\n",
       "      <td>LithoCluster_3400</td>\n",
       "      <td>98601/01971</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dev</td>\n",
       "      <td>2023-01-17 10:00:00</td>\n",
       "      <td>2023-01-17 11:00:00</td>\n",
       "      <td>thiam14</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2646</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2647</th>\n",
       "      <td>PLINE300</td>\n",
       "      <td>LithoCluster_3400</td>\n",
       "      <td>31199/01514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JSR</td>\n",
       "      <td>2023-01-21 19:00:00</td>\n",
       "      <td>2023-01-21 20:00:00</td>\n",
       "      <td>furuka28</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2647</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648</th>\n",
       "      <td>PLINE300</td>\n",
       "      <td>LithoCluster_3400</td>\n",
       "      <td>31199/00653</td>\n",
       "      <td>NaN</td>\n",
       "      <td>source calibration by ASML</td>\n",
       "      <td>2023-01-10 23:00:00</td>\n",
       "      <td>2023-01-11 00:00:00</td>\n",
       "      <td>versluij</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2648</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2649</th>\n",
       "      <td>PLINE300</td>\n",
       "      <td>LithoCluster_3400</td>\n",
       "      <td>31199/01413</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-16 12:51:00</td>\n",
       "      <td>2023-01-16 13:20:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vdries51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2649</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.933333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2650</th>\n",
       "      <td>PLINE300</td>\n",
       "      <td>LithoCluster_3400</td>\n",
       "      <td>31199/31811</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-20 06:25:00</td>\n",
       "      <td>2023-01-20 07:02:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sterckxg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2650</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.466667</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2651 rows  19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      FACILITY               Tool          WBS  Cluster  \\\n",
       "0     PLINE200        300MM_DICER  32299/02016      0.0   \n",
       "1     PLINE200        300MM_DICER  32299/02016      1.0   \n",
       "2     PLINE200        300MM_DICER  36671/01224      0.0   \n",
       "3     PLINE200          ACT12_MTM  37004/37141      0.0   \n",
       "4     PLINE200          ACT12_MTM  37004/37141      1.0   \n",
       "...        ...                ...          ...      ...   \n",
       "2646  PLINE300  LithoCluster_3400  98601/01971      NaN   \n",
       "2647  PLINE300  LithoCluster_3400  31199/01514      NaN   \n",
       "2648  PLINE300  LithoCluster_3400  31199/00653      NaN   \n",
       "2649  PLINE300  LithoCluster_3400  31199/01413      NaN   \n",
       "2650  PLINE300  LithoCluster_3400  31199/31811      NaN   \n",
       "\n",
       "                          Description               Begin                 End  \\\n",
       "0                            Prio lot 2023-01-16 08:25:00 2023-01-16 17:00:00   \n",
       "1     For Alginment issue lot process 2023-01-19 09:30:00 2023-01-19 16:48:00   \n",
       "2       W2W bonding ET overlay backup 2023-01-13 09:30:00 2023-01-13 13:27:00   \n",
       "3                                 DFR 2023-01-12 09:00:00 2023-01-12 13:30:00   \n",
       "4                                 DFR 2023-01-20 09:00:00 2023-01-20 13:49:00   \n",
       "...                               ...                 ...                 ...   \n",
       "2646                              dev 2023-01-17 10:00:00 2023-01-17 11:00:00   \n",
       "2647                              JSR 2023-01-21 19:00:00 2023-01-21 20:00:00   \n",
       "2648       source calibration by ASML 2023-01-10 23:00:00 2023-01-11 00:00:00   \n",
       "2649                              NaN 2023-01-16 12:51:00 2023-01-16 13:20:00   \n",
       "2650                              NaN 2023-01-20 06:25:00 2023-01-20 07:02:00   \n",
       "\n",
       "     IIO_User_id Fab300_User_id Modules  Fab300_Duration  IIO_Duration  \\\n",
       "0        schoofs        schoofs                 8.566667           7.5   \n",
       "1         hiro13         hiro13                 7.266667           6.5   \n",
       "2       ameghn57       ameghn57                 2.833333           3.0   \n",
       "3       eswara57       cooman32                 1.516667           4.5   \n",
       "4       eswara57       haever53                 3.816667           4.0   \n",
       "...          ...            ...     ...              ...           ...   \n",
       "2646     thiam14            NaN                      NaN           1.0   \n",
       "2647    furuka28            NaN                      NaN           1.0   \n",
       "2648    versluij            NaN                      NaN           1.0   \n",
       "2649         NaN       vdries51     NaN         0.483333           NaN   \n",
       "2650         NaN       sterckxg     NaN         0.616667           NaN   \n",
       "\n",
       "      TR_Cluster    id  BillableDuration  BillableDuration1  \\\n",
       "0            NaN     0              34.0          34.333333   \n",
       "1            NaN     1              29.0          29.200000   \n",
       "2            NaN     2              16.0          15.800000   \n",
       "3            NaN     3              18.0          18.000000   \n",
       "4            NaN     4              19.0          19.266667   \n",
       "...          ...   ...               ...                ...   \n",
       "2646         NaN  2646               4.0           4.000000   \n",
       "2647         NaN  2647               4.0           4.000000   \n",
       "2648         NaN  2648               4.0           4.000000   \n",
       "2649         NaN  2649               2.0           1.933333   \n",
       "2650         NaN  2650               2.0           2.466667   \n",
       "\n",
       "      BillableDuration2  BillableDuration3  BillableDuration4  \n",
       "0                  34.0               8.50               34.0  \n",
       "1                  29.0               7.25               29.0  \n",
       "2                  16.0               4.00               16.0  \n",
       "3                  18.0               4.50               18.0  \n",
       "4                  19.0               4.75               19.0  \n",
       "...                 ...                ...                ...  \n",
       "2646                4.0               1.00                4.0  \n",
       "2647                4.0               1.00                4.0  \n",
       "2648                4.0               1.00                4.0  \n",
       "2649                2.0               0.50                2.0  \n",
       "2650                2.0               0.50                2.0  \n",
       "\n",
       "[2651 rows x 19 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d7d61869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677d7a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "= (tbl as table) =>\n",
    "        let\n",
    "            x = Table.Buffer(Table.Sort(tbl, {{\"Datim\", Order.Ascending}})),\n",
    "            FirstRow = Table.First(x),\n",
    "            FullDuration = FirstRow[Duration],\n",
    "            NumberOfRows = Table.RowCount(x),\n",
    "            GoodBadDuration =\n",
    "                if NumberOfRows = 1 then\n",
    "                    if FirstRow[State] = \"UP\" then\n",
    "                        FullDuration\n",
    "                    else\n",
    "                        0\n",
    "                else\n",
    "                    if NumberOfRows = 2 then\n",
    "                        if FirstRow[State] = \"UP\" then // went down during the reservation\n",
    "                            0\n",
    "                        else\n",
    "                            if Duration.TotalHours(Table.Last(x)[Datim] - FirstRow[Datim]) / FullDuration < 0.5 then\n",
    "                                FullDuration - Duration.TotalHours(Table.Last(x)[Datim] - FirstRow[Datim])\n",
    "                            else\n",
    "                                0\n",
    "                    else\n",
    "                        0\n",
    "        in\n",
    "            GoodBadDuration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

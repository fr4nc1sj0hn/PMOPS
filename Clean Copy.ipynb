{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d33bc10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools as ft\n",
    "#remove repeats\n",
    "from datetime import date,datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0679a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d264de6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "798d9201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_reservations(df):\n",
    "    try:\n",
    "        Fab300_raw_reservations = df.copy()\n",
    "        Fab300_raw_reservations[\"DATE_TIME_STAMP\"] = pd.to_datetime(Fab300_raw_reservations[\"DATE_TIME_STAMP\"])\n",
    "        SortedRows = Fab300_raw_reservations.sort_values([\"EVENT_ROW_ID\"])\n",
    "\n",
    "        index = range(1,len(SortedRows)+1)\n",
    "        IndexShift_1 = [i-1 if i%2 != 0 else np.nan for i in index]\n",
    "        IndexShift_2 = [i-1 if i%2 == 0 else np.nan for i in index]\n",
    "\n",
    "        SortedRows[\"Index\"] = index\n",
    "        SortedRows[\"IndexShift_1\"] = IndexShift_1\n",
    "        SortedRows[\"IndexShift_2\"] = IndexShift_2\n",
    "        SortedRows[\"IndexShift_1_1\"] = SortedRows[\"IndexShift_1\"].fillna(method='bfill')\n",
    "        SortedRows[\"IndexShift_2_1\"] = SortedRows[\"IndexShift_2\"].fillna(method='bfill')\n",
    "        SortedRows = SortedRows.drop(columns=['IndexShift_1', 'IndexShift_2'])\n",
    "        SortedRows = SortedRows.rename(columns={\"IndexShift_1_1\":\"IndexShift_1\",\"IndexShift_2_1\":\"IndexShift_2\"})\n",
    "        FilledUp2 = SortedRows.copy()\n",
    "\n",
    "        UnpivotedOnlySelectedColumns = pd.melt(FilledUp2, id_vars=['FO_ROW_ID','Index','IndexShift_1','IndexShift_2'], \n",
    "                    value_vars=[\"ResWBS\", \"ResTk\", \"DATE_TIME_STAMP\", \"USER_ID\", \"EVENT_ROW_ID\"])\n",
    "\n",
    "        UnpivotedOnlySelectedColumns[\"ResProperty_1\"] = np.where(UnpivotedOnlySelectedColumns['Index']==UnpivotedOnlySelectedColumns['IndexShift_1'], UnpivotedOnlySelectedColumns['variable'] + \"_Start\", UnpivotedOnlySelectedColumns['variable'] + \"_End\")\n",
    "        UnpivotedOnlySelectedColumns[\"ResProperty_2\"] = np.where(UnpivotedOnlySelectedColumns['Index']==UnpivotedOnlySelectedColumns['IndexShift_2'], UnpivotedOnlySelectedColumns['variable'] + \"_Start\", UnpivotedOnlySelectedColumns['variable'] + \"_End\")\n",
    "\n",
    "        AddedCustom4 = UnpivotedOnlySelectedColumns.copy()\n",
    "        RemovedColumns1 = AddedCustom4.drop(columns=[\"Index\", \"IndexShift_2\", \"variable\", \"ResProperty_2\"])\n",
    "\n",
    "        PivotedColumn1 = RemovedColumns1.pivot(index=['FO_ROW_ID','IndexShift_1'],columns='ResProperty_1',values='value').reset_index()\n",
    "\n",
    "        if \"ResWBS_Start\" in PivotedColumn1.columns:\n",
    "            FilteredRows01 = PivotedColumn1[(PivotedColumn1[\"ResWBS_Start\"].notnull() & PivotedColumn1['ResWBS_Start'].str.len() > 0)]\n",
    "        else:\n",
    "            #empty dataframe\n",
    "            FilteredRows01 = pd.DataFrame(columns=PivotedColumn1.columns)\n",
    "            \n",
    "        \n",
    "        RemovedColumns2 = AddedCustom4.drop(columns=[\"Index\", \"IndexShift_1\", \"variable\", \"ResProperty_1\"])\n",
    "        PivotedColumn2 = RemovedColumns2.pivot(index=['FO_ROW_ID','IndexShift_2'],columns='ResProperty_2',values='value').reset_index()\n",
    "\n",
    "        FilteredRows02 = PivotedColumn2[(PivotedColumn2[\"ResWBS_Start\"].notnull() & PivotedColumn2['ResWBS_Start'].str.len() > 0)]\n",
    "        \n",
    "        if \"ResWBS_Start\" in PivotedColumn2.columns:\n",
    "            FilteredRows02 = PivotedColumn2[(PivotedColumn2[\"ResWBS_Start\"].notnull() & PivotedColumn2['ResWBS_Start'].str.len() > 0)]\n",
    "        else:\n",
    "            #empty dataframe\n",
    "            FilteredRows02 = pd.DataFrame(columns=PivotedColumn2.columns)\n",
    "        \n",
    "        columns = [\n",
    "            'FO_ROW_ID', \n",
    "            'DATE_TIME_STAMP_End',\n",
    "            'DATE_TIME_STAMP_Start', \n",
    "            'EVENT_ROW_ID_End', \n",
    "            'EVENT_ROW_ID_Start',\n",
    "            'ResTk_End', \n",
    "            'ResTk_Start', \n",
    "            'ResWBS_End', \n",
    "            'ResWBS_Start', \n",
    "            'USER_ID_End',\n",
    "            'USER_ID_Start'\n",
    "        ]\n",
    "            \n",
    "            \n",
    "        if FilteredRows01.shape[0] == 0:\n",
    "            combined = FilteredRows02[columns]\n",
    "        elif FilteredRows02.shape[0] == 0:\n",
    "            combined = FilteredRows01[columns]\n",
    "        else:\n",
    "            FilteredRows01 = FilteredRows01[columns]\n",
    "            FilteredRows02 = FilteredRows02[columns]\n",
    "        \n",
    "        combined = pd.concat([FilteredRows01,FilteredRows02])\n",
    "\n",
    "        combined = combined.rename(\n",
    "            columns=\n",
    "            {\n",
    "                \"ResTk_Start\":\"ResTk\",\n",
    "                \"ResWBS_Start\":\"WBS\",\n",
    "                \"EVENT_ROW_ID_Start\": \"EVENT_ROW_ID_Begin\",\n",
    "                \"DATE_TIME_STAMP_Start\": \"DATE_TIME_STAMP_Begin\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        columns = [\n",
    "            \"FO_ROW_ID\",\n",
    "            \"ResTk\", \n",
    "            \"WBS\", \n",
    "            \"EVENT_ROW_ID_Begin\", \n",
    "            \"EVENT_ROW_ID_End\", \n",
    "            \"DATE_TIME_STAMP_Begin\", \n",
    "            \"DATE_TIME_STAMP_End\", \n",
    "            \"USER_ID_Start\", \n",
    "            \"USER_ID_End\"\n",
    "        ]\n",
    "        Tools_with_resersvations  = combined#[columns]\n",
    "\n",
    "        final = combined[columns]\n",
    "        return final\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def processFab300RawReservations(df):\n",
    "    columns = [\n",
    "        \"FO_ROW_ID\",\n",
    "        \"DATE_TIME_STAMP_Begin\",\n",
    "        \"DATE_TIME_STAMP_End\",\n",
    "        'EVENT_ROW_ID_Begin', \n",
    "        'EVENT_ROW_ID_End',\n",
    "        'ResTk',\n",
    "        'USER_ID_Start',\n",
    "        'USER_ID_End',\n",
    "        'WBS'\n",
    "    ]\n",
    "\n",
    "    for_row_ids = df[\"FO_ROW_ID\"].unique()\n",
    "\n",
    "    Tools_with_reservations = pd.DataFrame(columns=columns)\n",
    "    for row_id in for_row_ids:\n",
    "        grpdata = df[df[\"FO_ROW_ID\"] == row_id]\n",
    "\n",
    "        df_reserv = identify_reservations(grpdata)\n",
    "        if df.shape[0] > 0:\n",
    "            Tools_with_reservations = pd.concat([Tools_with_reservations,df_reserv])\n",
    "\n",
    "\n",
    "    return Tools_with_reservations\n",
    "\n",
    "\n",
    "def FAB300_with_tool_names(Tools_with_reservations,Tools_Parents):\n",
    "    Expanded_Tools_parents = pd.merge(\n",
    "        Tools_with_reservations, \n",
    "        Tools_Parents, \n",
    "        left_on=[\"FO_ROW_ID\"], \n",
    "        right_on=[\"ROW_ID\"], \n",
    "        how=\"left\",\n",
    "        suffixes=[\"\",\"_y\"]\n",
    "    )\n",
    "    Expanded_Tools_parents = Expanded_Tools_parents.rename(\n",
    "        columns={\n",
    "            \"ENT_NAME\":\"Tool\",\n",
    "            \"USER_ID_Start\":\"USER_ID_Begin\",\n",
    "            \"USER_ID_End\":\"USER_ID_End\"\n",
    "        }\n",
    "    )\n",
    "    Expanded_Tools_parents_filteredRows = Expanded_Tools_parents[\n",
    "        Expanded_Tools_parents[\"USER_ID_Begin\"] == Expanded_Tools_parents[\"USER_ID_End\"]\n",
    "    ]\n",
    "    Expanded_Tools_parents_filteredRows = Expanded_Tools_parents_filteredRows.rename(\n",
    "        columns={\n",
    "            \"USER_ID_Begin\":\"User_id\",\n",
    "            \"DATE_TIME_STAMP_Begin\":\"Begin\",\n",
    "            \"DATE_TIME_STAMP_End\":\"End\"\n",
    "        }\n",
    "    )\n",
    "    Expanded_Tools_parents_filteredRows = Expanded_Tools_parents_filteredRows.sort_values([\"FO_ROW_ID\",\"EVENT_ROW_ID_Begin\"])\n",
    "    Fab300_Res_id = range(0,len(Expanded_Tools_parents_filteredRows))\n",
    "    Expanded_Tools_parents_filteredRows[\"Fab300_Res_id\"] = Fab300_Res_id\n",
    "    Expanded_Tools_parents_filteredRows = Expanded_Tools_parents_filteredRows[\n",
    "        (Expanded_Tools_parents_filteredRows[\"Tool\"].notnull())\n",
    "        & (Expanded_Tools_parents_filteredRows['Tool'].str.len() > 0)\n",
    "    ]\n",
    "    columns = [\n",
    "        \"FO_ROW_ID\",\n",
    "        \"EVENT_ROW_ID_Begin\",\n",
    "        \"Begin\",\n",
    "        \"End\",\n",
    "        \"Fab300_Res_id\",\n",
    "        \"FACILITY\",\n",
    "        \"ResTk\",\n",
    "        \"Tool\",\n",
    "        \"User_id\",\n",
    "        \"WBS\"\n",
    "    ]\n",
    "    Expanded_Tools_parents_filteredRows[columns]\n",
    "    FAB300_with_tool_names = Expanded_Tools_parents_filteredRows[columns]\n",
    "\n",
    "    #Sample for FO_ROW_ID == 76\n",
    "    return FAB300_with_tool_names\n",
    "\n",
    "\n",
    "#IIO_raw_reservations\n",
    "def IIO_without_modules(df):\n",
    "    df = df.reset_index()\n",
    "    df[\"Begin\"] = pd.to_datetime(df[\"Begin\"],format=\"%Y-%m-%d %H:%M\")\n",
    "    df[\"End\"] = pd.to_datetime(df[\"End\"],format=\"%Y-%m-%d %H:%M\")\n",
    "    df = df.sort_values([\"WBS\",\"Tool\",\"FACILITY\",\"Module\",\"Begin\"])\n",
    "    df[\"End_Down\"] = df[\"End\"].shift(1)\n",
    "    df[\"Adjacent_Down\"] = np.where(df['Begin']==df['End_Down'], True, False)\n",
    "    df[\"IndexCopy\"] = np.where(df[\"Adjacent_Down\"] == True,np.nan,df[\"index\"])\n",
    "    df[\"IndexCopy2\"] = df[\"IndexCopy\"].fillna(method='ffill')\n",
    "    df[\"Module\"] = df[\"Module\"].fillna(\"\")\n",
    "    df[\"Description\"] = df[\"Description\"].fillna(\"\")\n",
    "    df = df[['WBS','FACILITY', 'Module','Tool','Begin', 'Description', 'End', 'User_id','IndexCopy2','End_Down']]\n",
    "\n",
    "\n",
    "    params = {\n",
    "        'Begin': 'min',\n",
    "        'End': 'max',\n",
    "        'Description': lambda x: ';'.join(sorted(pd.Series.unique(x))),\n",
    "        'User_id': lambda x: ';'.join(sorted(pd.Series.unique(x)))\n",
    "    }\n",
    "    sub = df[[\"WBS\",\"Tool\",\"FACILITY\",\"Module\",\"Begin\",\"End\",\"Description\",'User_id','IndexCopy2']]\n",
    "    GroupedRows1 = sub.groupby([\"WBS\",\"Tool\",\"FACILITY\",\"Module\",'IndexCopy2']).agg(params).reset_index()\n",
    "\n",
    "    params = {\n",
    "        'Module': lambda x: ';'.join(sorted(pd.Series.unique(x))),\n",
    "        'Description': 'first',\n",
    "        'User_id': 'first'\n",
    "    }\n",
    "    \n",
    "\n",
    "    GroupedRows1 = GroupedRows1.groupby([\"WBS\",\"Tool\",\"FACILITY\",\"Begin\", \"End\"]).agg(params).reset_index()\n",
    "    GroupedRows1 = GroupedRows1.sort_values([\"FACILITY\", \"Tool\", \"WBS\", \"Begin\", \"End\"])\n",
    "\n",
    "    Index = range(0,len(GroupedRows1))\n",
    "    GroupedRows1[\"IIO_Res_id\"] = Index\n",
    "    \n",
    "    GroupedRows1 = GroupedRows1.rename(\n",
    "        columns={\n",
    "            \"Module\":\"Modules\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    IIO_without_modules = GroupedRows1.copy()\n",
    "    return IIO_without_modules\n",
    "\n",
    "\n",
    "def Fab300_iio_merger(df):\n",
    "    df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])\n",
    "    df = df.sort_values([\"WBS\",\"FACILITY\",'Tool',\"DateTime\",\"FAB300_BeginEnd\"])\n",
    "\n",
    "    df[\"Fab300_Res_id_UP\"] = df[\"Fab300_Res_id\"].fillna(method='bfill')\n",
    "    df[\"Fab300_Res_id_DOWN\"] = df[\"Fab300_Res_id\"].fillna(method='ffill')\n",
    "    df[\"IIO_Res_id_UP\"] = df[\"IIO_Res_id\"].fillna(method='bfill')\n",
    "    df[\"IIO_Res_id_DOWN\"] = df[\"IIO_Res_id\"].fillna(method='ffill')\n",
    "\n",
    "    fab_iio_Filtered_Rows = df[\n",
    "        (df[\"Fab300_Res_id_UP\"] == df[\"Fab300_Res_id_DOWN\"])\n",
    "        & (df[\"IIO_Res_id_UP\"] == df[\"IIO_Res_id_DOWN\"])\n",
    "        & (df[\"Fab300_Res_id_UP\"].notnull())\n",
    "        & (df[\"IIO_Res_id_UP\"].notnull())\n",
    "    ] \n",
    "    \n",
    "    Removed_Other_Columns = fab_iio_Filtered_Rows[[\"Fab300_Res_id_UP\", \"IIO_Res_id_UP\"]]\n",
    "    Removed_Duplicates = Removed_Other_Columns.drop_duplicates()\n",
    "    Renamed_Columns = Removed_Duplicates.rename(\n",
    "        columns = {\n",
    "            \"Fab300_Res_id_UP\":\"Fab300_Res_id\",\n",
    "            \"IIO_Res_id_UP\":\"IIO_Res_id\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    index = range(0,len(Renamed_Columns))\n",
    "    Renamed_Columns[\"Index\"] = index\n",
    "    Added_Index = Renamed_Columns.copy()\n",
    "    State_DOWN_Remove = Added_Index.copy()\n",
    "    State_DOWN_Remove.loc[-1] = [np.nan,np.nan,-1]  # adding a row\n",
    "    State_DOWN_Insert = State_DOWN_Remove.copy()\n",
    "    State_DOWN_Insert = State_DOWN_Insert.sort_values([\"Index\"])\n",
    "    index2 = range(0,len(State_DOWN_Insert))\n",
    "    State_DOWN_Insert[\"Index2\"] = index2\n",
    "\n",
    "    State_DOWN_Add_Index = State_DOWN_Insert.copy()\n",
    "    State_DOWN_Rename = State_DOWN_Add_Index.rename(\n",
    "        columns = {\n",
    "            \"Fab300_Res_id\":\"Fab300_Res_id_DOWN\",\n",
    "            \"IIO_Res_id\":\"IIO_Res_id_DOWN\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    With_DOWN = Added_Index.merge(\n",
    "        State_DOWN_Rename, \n",
    "        left_on=[\"Index\"],\n",
    "        right_on=[\"Index2\"],\n",
    "        suffixes=[\"\",\"_y\"],\n",
    "        how = 'left'\n",
    "    )\n",
    "\n",
    "    With_DOWN[\"Index3\"] = np.where(\n",
    "        (With_DOWN['Fab300_Res_id']==With_DOWN['Fab300_Res_id_DOWN'])\n",
    "        | (With_DOWN['IIO_Res_id']==With_DOWN['IIO_Res_id_DOWN']), \n",
    "        np.nan,\n",
    "         With_DOWN['Index']\n",
    "    )\n",
    "\n",
    "\n",
    "    Replaced_Value = With_DOWN.copy()\n",
    "    Replaced_Value[\"Index4\"] = Replaced_Value[\"Index3\"].fillna(method='ffill')\n",
    "    Replaced_Value = Replaced_Value.drop(columns=[\"Fab300_Res_id_DOWN\", \"IIO_Res_id_DOWN\"])\n",
    "    Renamed_Columns1 = Replaced_Value.rename(\n",
    "    columns={\n",
    "        \"Index4\":\"Cluster\"\n",
    "    })\n",
    "\n",
    "\n",
    "    Merged_queries = Renamed_Columns1.merge(\n",
    "        Source_fab, \n",
    "        left_on=[\"Fab300_Res_id\"],\n",
    "        right_on=[\"Fab300_Res_id\"],\n",
    "        suffixes=[\"\",\"_y\"],\n",
    "        how = 'left'\n",
    "    )\n",
    "    Expanded_Fab300_with_tool_names = Merged_queries.rename(\n",
    "    columns ={\n",
    "        \"Begin\": \"Fab300_Begin\", \n",
    "        \"End\": \"Fab300_End\", \n",
    "        \"User_id\": \"Fab300_User_id\"\n",
    "    })\n",
    "    Merged_queries_1 = Expanded_Fab300_with_tool_names.merge(\n",
    "        Source_iio, \n",
    "        left_on=[\"IIO_Res_id\"],\n",
    "        right_on=[\"IIO_Res_id\"],\n",
    "        suffixes=[\"\",\"_y\"],\n",
    "        how = 'left'\n",
    "    )\n",
    "    Expanded_IIO_without_modules = Merged_queries_1.rename(\n",
    "    columns ={\n",
    "        \"Begin\": \"IIO_Begin\", \n",
    "        \"End\": \"IIO_End\", \n",
    "        \"Modules\": \"Modules\",\n",
    "        \"User_id\": \"IIO_User_id\",\n",
    "        \"Description\": \"Description\"\n",
    "    })\n",
    "    Expanded_IIO_without_modules = Expanded_IIO_without_modules.drop(\n",
    "        columns=[\"FACILITY_y\",\"Tool_y\",\"WBS_y\",'Index', 'Index_y', 'Index2', 'Index3']\n",
    "    )\n",
    "    \n",
    "    return Expanded_IIO_without_modules\n",
    "\n",
    "\n",
    "def Final_Fab300_IIO_reservations(IIO_without_modules,Fab300withtoolnames,Fab300_IIO_overlaps_ids):\n",
    "    Fab300_IIO_overlaps_ids[\"Fab300_Begin\"] = pd.to_datetime(Fab300_IIO_overlaps_ids[\"Fab300_Begin\"],format=\"%Y-%m-%d %H:%M\")\n",
    "    Fab300_IIO_overlaps_ids[\"Fab300_End\"] = pd.to_datetime(Fab300_IIO_overlaps_ids[\"Fab300_End\"],format=\"%Y-%m-%d %H:%M\")\n",
    "    Fab300_IIO_overlaps_ids[\"IIO_Begin\"] = pd.to_datetime(Fab300_IIO_overlaps_ids[\"IIO_Begin\"],format=\"%Y-%m-%d %H:%M\")\n",
    "    Fab300_IIO_overlaps_ids[\"IIO_End\"] = pd.to_datetime(Fab300_IIO_overlaps_ids[\"IIO_End\"],format=\"%Y-%m-%d %H:%M\")\n",
    "    \n",
    "    AddedCustom = Fab300_IIO_overlaps_ids.copy()\n",
    "\n",
    "    AddedCustom['Modules'] = AddedCustom['Modules'].fillna('')\n",
    "    AddedCustom[\"Fab300_Duration\"] = (AddedCustom[\"Fab300_End\"] - AddedCustom[\"Fab300_Begin\"])/np.timedelta64(1, 'h')\n",
    "    AddedCustom1 = AddedCustom.copy()\n",
    "    AddedCustom1[\"IIO_Duration\"] = (AddedCustom1[\"IIO_End\"] - AddedCustom1[\"IIO_Begin\"])/np.timedelta64(1, 'h')\n",
    "    params = {\n",
    "        'Fab300_Begin': 'min',\n",
    "        'IIO_Begin': 'min',\n",
    "        'Fab300_End': 'max',\n",
    "        'IIO_End': 'max',\n",
    "        'Fab300_Res_id': 'count',\n",
    "        'Description': lambda x: ';'.join(sorted(pd.Series.unique(x))),\n",
    "        'IIO_User_id': lambda x: ';'.join(sorted(pd.Series.unique(x))),\n",
    "        'Fab300_User_id': lambda x: ';'.join(sorted(pd.Series.unique(x))),\n",
    "        'Modules': lambda x: ';'.join(sorted(pd.Series.unique(x)))\n",
    "    }\n",
    "    sub = AddedCustom1[\n",
    "        [\n",
    "            \"FACILITY\", \n",
    "            \"Tool\", \n",
    "            \"WBS\", \n",
    "            \"Cluster\",\n",
    "            \"Fab300_Begin\",\n",
    "            \"IIO_Begin\",\n",
    "            'Fab300_End',\n",
    "            'IIO_End',\n",
    "            'Fab300_Res_id',\n",
    "            'IIO_Res_id',\n",
    "            'Description',\n",
    "            'IIO_User_id',\n",
    "            'Fab300_User_id',\n",
    "            'Modules'\n",
    "        ]\n",
    "    ]\n",
    "    GroupedRows1 = sub.groupby([\"FACILITY\", \"Tool\", \"WBS\", \"Cluster\"]).agg(params).reset_index()\n",
    "    GroupedRows1[\"Begin\"] = np.where(GroupedRows1['Fab300_Begin']>GroupedRows1['IIO_Begin'], GroupedRows1['IIO_Begin'], GroupedRows1['Fab300_Begin'])\n",
    "    GroupedRows1[\"End\"]  = np.where(GroupedRows1['Fab300_End']>GroupedRows1['IIO_End'], GroupedRows1['Fab300_End'], GroupedRows1['IIO_End'])\n",
    "    GroupedRows1 = GroupedRows1.rename(\n",
    "    columns={\n",
    "        \"Fab300_Res_id\": \"Cnt\"\n",
    "    }).drop(columns=['Fab300_Begin','Fab300_End','IIO_Begin','IIO_End'])\n",
    "\n",
    "\n",
    "    Duration_FAB = AddedCustom1[\n",
    "        [\n",
    "            \"FACILITY\", \n",
    "            \"Tool\", \n",
    "            \"WBS\", \n",
    "            \"Cluster\",\n",
    "            \"Fab300_Res_id\",\n",
    "            'Fab300_Duration'\n",
    "        ]\n",
    "    ].drop_duplicates().reset_index()\n",
    "\n",
    "    Duration_IIO = AddedCustom1[\n",
    "        [\n",
    "            \"FACILITY\", \n",
    "            \"Tool\", \n",
    "            \"WBS\", \n",
    "            \"Cluster\",\n",
    "            \"IIO_Res_id\",\n",
    "            'IIO_Duration'\n",
    "        ]\n",
    "    ].drop_duplicates().reset_index()\n",
    "\n",
    "    GroupedRows1_FAB = Duration_FAB.groupby([\"FACILITY\", \"Tool\", \"WBS\", \"Cluster\"]).sum().reset_index()\n",
    "    GroupedRows1_IIO = Duration_IIO.groupby([\"FACILITY\", \"Tool\", \"WBS\", \"Cluster\"]).sum().reset_index()\n",
    "\n",
    "\n",
    "    GroupedRows = pd.concat(\n",
    "        objs=(iDF.set_index([\"FACILITY\", \"Tool\", \"WBS\", \"Cluster\"]) for iDF in (GroupedRows1, GroupedRows1_FAB, GroupedRows1_IIO)),\n",
    "        axis=1, \n",
    "        join='inner'\n",
    "    ).reset_index()\n",
    "    GroupedRows['FAB_IIO_Ratio'] = GroupedRows[\"Fab300_Duration\"] / GroupedRows[\"IIO_Duration\"]\n",
    "\n",
    "    GroupedRows = GroupedRows[\n",
    "        [\n",
    "            'FACILITY', \n",
    "            'Tool', \n",
    "            'WBS', \n",
    "            'Cluster', \n",
    "            'Cnt', \n",
    "            'Description',\n",
    "            'Begin', \n",
    "            'End',\n",
    "            'IIO_User_id', \n",
    "            'Fab300_User_id', \n",
    "            'Modules',\n",
    "            'Fab300_Duration', \n",
    "            'IIO_Duration', \n",
    "            'FAB_IIO_Ratio'\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    FilteredRows = GroupedRows[\n",
    "        (GroupedRows[\"FAB_IIO_Ratio\"] < 3.0)\n",
    "        & (GroupedRows[\"Cnt\"] <= 4)\n",
    "    ]\n",
    "\n",
    "    Fab300_IIO_valid_combos = FilteredRows.drop(columns = [\"Cnt\"])\n",
    "\n",
    "    outer = Fab300_IIO_overlaps_ids.merge(\n",
    "        Fab300_IIO_valid_combos, \n",
    "        how='outer',\n",
    "        left_on = [\"FACILITY\", \"Tool\", \"WBS\", \"Cluster\"],\n",
    "        right_on = [\"FACILITY\", \"Tool\", \"WBS\", \"Cluster\"],\n",
    "        indicator=True\n",
    "    )\n",
    "    Fab300_IIO_bad_combos = outer[(outer._merge=='left_only')].drop('_merge', axis=1)\n",
    "\n",
    "\n",
    "    IIO_ids_from_bad_combos = Fab300_IIO_bad_combos[[\"IIO_Res_id\"]]\n",
    "    RemovedDuplicates = IIO_ids_from_bad_combos.drop_duplicates()\n",
    "\n",
    "    MergedQueries = RemovedDuplicates.merge(\n",
    "        IIO_without_modules, \n",
    "        how='inner',\n",
    "        left_on = [\"IIO_Res_id\"],\n",
    "        right_on = [\"IIO_Res_id\"],\n",
    "        indicator=True\n",
    "    )\n",
    "    RemovedColumns1 = MergedQueries.drop(columns=[\"IIO_Res_id\"])\n",
    "    IIO_from_bad_combos = RemovedColumns1.rename(\n",
    "    columns = {\n",
    "        'User_id':'IIO_User_id'\n",
    "    }).drop(columns=[\"_merge\"])\n",
    "\n",
    "    outer_IIO_from_bad_combos = IIO_without_modules.merge(\n",
    "        Fab300_IIO_overlaps_ids, \n",
    "        how='outer',\n",
    "        left_on = [\"IIO_Res_id\"],\n",
    "        right_on = [\"IIO_Res_id\"],\n",
    "        indicator=True\n",
    "    )\n",
    "    Pure_IIO_Join = outer_IIO_from_bad_combos[(outer_IIO_from_bad_combos._merge=='left_only')].drop('_merge', axis=1)\n",
    "    Pure_IIO = Pure_IIO_Join[\n",
    "        [\n",
    "            'Begin', \n",
    "            'Description_x', \n",
    "            'End', \n",
    "            'FACILITY_x', \n",
    "            'IIO_Res_id',\n",
    "            'Modules_x', \n",
    "            'Tool_x', \n",
    "            'User_id', \n",
    "            'WBS_x'\n",
    "        ]\n",
    "    ]\n",
    "    RenamedColumns = Pure_IIO.rename(\n",
    "        columns = {\n",
    "            'User_id':'IIO_User_id',\n",
    "            'Description_x': 'Description',\n",
    "            'FACILITY_x': 'FACILITY',\n",
    "            'Modules_x': 'Modules',\n",
    "            'Tool_x': 'Tool',\n",
    "            'WBS_x': 'WBS'\n",
    "        }\n",
    "    )\n",
    "    AppendedQuery = pd.concat([RenamedColumns,IIO_from_bad_combos]).drop(columns=['IIO_Res_id'])\n",
    "    AppendedQuery[\"Begin\"] = pd.to_datetime(AppendedQuery[\"Begin\"],format=\"%Y-%m-%d %H:%M\")\n",
    "    AppendedQuery[\"End\"] = pd.to_datetime(AppendedQuery[\"End\"],format=\"%Y-%m-%d %H:%M\")\n",
    "    AppendedQuery[\"IIO_Duration\"] = (AppendedQuery[\"End\"] - AppendedQuery[\"Begin\"])/np.timedelta64(1, 'h')\n",
    "    AddedCustom2 = AppendedQuery.copy()\n",
    "    AppendedQuery2 = pd.concat([Fab300_IIO_valid_combos,AddedCustom2])\n",
    "\n",
    "    outer_Fab300withtoolnames = Fab300withtoolnames.merge(\n",
    "        Fab300_IIO_overlaps_ids, \n",
    "        how='outer',\n",
    "        left_on = [\"Fab300_Res_id\"],\n",
    "        right_on = [\"Fab300_Res_id\"],\n",
    "        indicator=True\n",
    "    )\n",
    "\n",
    "    Pure_Fab300_Join = outer_Fab300withtoolnames[(outer_Fab300withtoolnames._merge=='left_only')].drop('_merge', axis=1)\n",
    "    Pure_Fab300 = Pure_Fab300_Join[\n",
    "        [\n",
    "            'Begin', \n",
    "            'End', \n",
    "            'Fab300_Res_id', \n",
    "            'FACILITY_x', \n",
    "            'Tool_x',\n",
    "            'User_id',\n",
    "            'WBS_x'\n",
    "        ]\n",
    "    ].drop(columns=['Fab300_Res_id']).rename(\n",
    "    columns={\n",
    "            'FACILITY_x': 'FACILITY',\n",
    "            'Tool_x': 'Tool',\n",
    "            'User_id': 'Fab300_User_id',\n",
    "            'WBS_x': 'WBS'\n",
    "    })\n",
    "    RenamedColumns1 = Pure_Fab300.copy()\n",
    "\n",
    "    RenamedColumns1[\"Begin\"] = pd.to_datetime(RenamedColumns1[\"Begin\"],format=\"%Y-%m-%d %H:%M\")\n",
    "    RenamedColumns1[\"End\"] = pd.to_datetime(RenamedColumns1[\"End\"],format=\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    RenamedColumns1[\"Fab300_Duration\"] = (RenamedColumns1[\"End\"] - RenamedColumns1[\"Begin\"])/np.timedelta64(1, 'h')\n",
    "    AddedCustom3 = RenamedColumns1.copy()\n",
    "\n",
    "    AppendedQuery3 = pd.concat([AppendedQuery2,AddedCustom3]).drop(columns=['FAB_IIO_Ratio'])\n",
    "    return AppendedQuery3\n",
    "\n",
    "\n",
    "def GetOverlapIndicator(row):\n",
    "    if (row[\"Scanners.Begin\"] is pd.NaT):\n",
    "        return \"No overlap\"\n",
    "    else:\n",
    "        if row[\"Scanners.Begin\"] < row[\"End\"] and row[\"Scanners.End\"] > row[\"Begin\"]:\n",
    "            if row[\"Begin\"] > row[\"Scanners.Begin\"] and row[\"End\"] < row[\"Scanners.End\"]:\n",
    "                return \"Useful overlap\"\n",
    "            else:\n",
    "                return \"Ignore overlap\"\n",
    "        else:\n",
    "            return \"No overlap\"\n",
    "\n",
    "\n",
    "def CheckForOverlaps(df):\n",
    "    Removed_Duplicates = df[\"Overlap\"].unique()\n",
    "    nb_of_rows = Removed_Duplicates.shape[0]\n",
    "    FirstValue = Removed_Duplicates[0]\n",
    "\n",
    "    if (nb_of_rows == 1) and (FirstValue == \"No overlap\"):\n",
    "        return True \n",
    "\n",
    "    return False\n",
    "\n",
    "def GetMinDate(row,col1, col2):\n",
    "    if (row[col1] is pd.NaT and row[col2] is not pd.NaT):\n",
    "        return row[col2]\n",
    "    elif (row[col1] is not pd.NaT and row[col2] is pd.NaT):\n",
    "        return row[col1]\n",
    "    else:\n",
    "        if row[col1] < row[col2]:\n",
    "            return row[col1]\n",
    "        else:\n",
    "            return row[col2]\n",
    "        \n",
    "def GetMaxDate(row,col1, col2):\n",
    "    if (row[col1] is pd.NaT and row[col2] is not pd.NaT):\n",
    "        return row[col2]\n",
    "    elif (row[col1] is not pd.NaT and row[col2] is pd.NaT):\n",
    "        return row[col1]\n",
    "    else:\n",
    "        if row[col1] < row[col2]:\n",
    "            return row[col2]\n",
    "        else:\n",
    "            return row[col1]\n",
    "\n",
    "\n",
    "def Final_Fab300_IIO_Reservations_clustered(df):\n",
    "    MergedQueries = df.merge(\n",
    "        LithoClusters, \n",
    "        how='left',\n",
    "        left_on = [\"Tool\"],\n",
    "        right_on = [\"ToolName\"]\n",
    "    )\n",
    "\n",
    "    print(\"MergedQueries\", MergedQueries.shape)\n",
    "    MergedQueries = MergedQueries.drop(columns=[\"ToolName\"])\n",
    "\n",
    "    SC_TR_filt = MergedQueries[\n",
    "        (MergedQueries[\"LithoCluster\"].notnull())\n",
    "        & (MergedQueries[\"LithoCluster\"] != \"\")\n",
    "    ]\n",
    "\n",
    "    SC_filt = SC_TR_filt[\n",
    "        SC_TR_filt[\"Tool\"].str.startswith(\"SC\")\n",
    "    ]\n",
    "    SC_filt['Tool'] = SC_filt['Tool'].str.replace('SC','LithoCluster_')\n",
    "    SC_to_Cluster = SC_filt.copy()\n",
    "    SC_index = SC_to_Cluster.copy()\n",
    "\n",
    "\n",
    "    index = range(0,len(SC_index))\n",
    "    SC_index[\"id\"] = index\n",
    "\n",
    "\n",
    "    TR_filt = SC_TR_filt[\n",
    "        SC_TR_filt[\"Tool\"].str.startswith(\"TR\")\n",
    "    ]\n",
    "    TR_index = TR_filt.copy()\n",
    "    index2 = range(0,len(TR_index))\n",
    "    TR_index[\"id\"] = index2\n",
    "\n",
    "\n",
    "    SC_index_sub = SC_index[\n",
    "        [\n",
    "            \"LithoCluster\", \"WBS\",\"Begin\", \"End\", \"id\"\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    TR_SC_Merge = TR_index.merge(\n",
    "        SC_index_sub, \n",
    "        how='left',\n",
    "        left_on = [\"LithoCluster\", \"WBS\"],\n",
    "        right_on = [\"LithoCluster\", \"WBS\"]\n",
    "    )\n",
    "    TR_SC_Expand = TR_SC_Merge.rename(\n",
    "    columns = {\n",
    "        \"Begin_y\":\"Scanners.Begin\",\n",
    "        \"End_y\":\"Scanners.End\",\n",
    "        \"Begin_x\":\"Begin\",\n",
    "        \"End_x\":\"End\",\n",
    "        \"id_y\":\"Scanners.id\",\n",
    "        \"id_x\":\"id\",\n",
    "        \"Cluster_x\": \"Cluster\",\n",
    "        'Description_x':\"Description\",\n",
    "        'Fab300_Duration_x':\"Fab300_Duration\",\n",
    "        'Fab300_User_id_x':\"Fab300_User_id\", \n",
    "        'FACILITY_x':\"FACILITY\", \n",
    "        'IIO_Duration_x':\"IIO_Duration\", \n",
    "        'IIO_User_id_x':\"IIO_User_id\",\n",
    "        'Modules_x':\"Modules\", \n",
    "        'Tool_x':\"Tool\"\n",
    "        \n",
    "    })\n",
    "\n",
    "    TR_SC_ovl_info = TR_SC_Expand.copy()\n",
    "    TR_SC_ovl_info[\"Overlap\"]  = TR_SC_ovl_info.apply(lambda row: GetOverlapIndicator(row),axis=1)\n",
    "\n",
    "    collated = pd.DataFrame(columns=[\n",
    "        'Begin', \n",
    "        'Cluster', \n",
    "        'Description', \n",
    "        'End', \n",
    "        'Fab300_Duration',\n",
    "        'Fab300_User_id', \n",
    "        'FACILITY', \n",
    "        'IIO_Duration', \n",
    "        'IIO_User_id', \n",
    "        'Modules',\n",
    "        'Tool', \n",
    "        'WBS', \n",
    "        'LithoCluster', \n",
    "        'id', \n",
    "        'Scanners.Begin', \n",
    "        'Scanners.End',\n",
    "        'Scanners.id', \n",
    "        'Overlap',\n",
    "        'Independent'\n",
    "    ])\n",
    "\n",
    "\n",
    "    ids = TR_SC_ovl_info[\"id\"].unique()\n",
    "\n",
    "    for id in ids:\n",
    "        grp = TR_SC_ovl_info[TR_SC_ovl_info[\"id\"] == id]\n",
    "        grp[\"Independent\"] = CheckForOverlaps(grp)\n",
    "        \n",
    "        collated = pd.concat([collated,grp])\n",
    "\n",
    "    TR_pure = collated[collated[\"Independent\"] == True]\n",
    "    TR_pure = TR_pure[[\"id\"]]\n",
    "\n",
    "    TR_merge  = TR_pure.merge(\n",
    "        TR_index, \n",
    "        how='inner',\n",
    "        left_on = [\"id\"],\n",
    "        right_on = [\"id\"]\n",
    "    )\n",
    "    TR_rmv = TR_merge.drop(columns=[\"id\"])\n",
    "\n",
    "\n",
    "    SC_filt_2 = TR_SC_ovl_info[\n",
    "        (TR_SC_ovl_info[\"Overlap\"] != \"No overlap\")\n",
    "        & (TR_SC_ovl_info[\"Overlap\"] != \"Ignore overlap\")\n",
    "    ]\n",
    "    SC_TR_columns = SC_filt_2[[\"Cluster\", \"Begin\", \"End\", \"Scanners.id\"]]\n",
    "\n",
    "    SC_TR_columns2 = SC_TR_columns[\n",
    "        [\n",
    "            \"Scanners.id\",\n",
    "            \"Begin\", \n",
    "            \"End\", \n",
    "            \"Cluster\"\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    SC_pure = SC_index.merge(\n",
    "        SC_TR_columns2, \n",
    "        how='left',\n",
    "        left_on = [\"id\"],\n",
    "        right_on = [\"Scanners.id\"]\n",
    "    )\n",
    "    SC_rename = SC_pure.rename(\n",
    "    columns={\n",
    "        \"Begin_y\": \"TR_Begin\", \n",
    "        \"End_y\": \"TR_End\", \n",
    "        \"Begin_x\": \"SC_Begin\", \n",
    "        \"End_x\": \"SC_End\", \n",
    "        \"Cluster_x\": \"Cluster\",\n",
    "        \"Cluster_y\": \"TR_Cluster\",\n",
    "        \n",
    "    })\n",
    "    SC_rename[\"Begin\"] = SC_rename.apply(lambda row: GetMinDate(row,\"SC_Begin\",\"TR_Begin\"),axis=1)\n",
    "    SC_rename[\"End\"] = SC_rename.apply(lambda row: GetMaxDate(row,\"SC_End\",\"TR_End\"),axis=1)\n",
    "    SC_final = SC_rename.drop(columns=[\"SC_Begin\", \"SC_End\", \"id\", \"TR_Begin\", \"TR_End\"])\n",
    "    NoClusters = MergedQueries[MergedQueries[\"LithoCluster\"].isna()]\n",
    "\n",
    "    Everything = pd.concat([NoClusters, TR_rmv])\n",
    "    Everything = pd.concat([Everything,SC_final])\n",
    "    Everything.drop_duplicates()\n",
    "\n",
    "    return Everything\n",
    "\n",
    "\n",
    "def ReducedState(row):\n",
    "    if row[\"State\"] in [\"UP\",\"PARTLY_UP\",\"RESERVED\"]:\n",
    "        return \"UP\"\n",
    "    elif row[\"State\"] == \"SPC_TEST\":\n",
    "        return \"SPC\"\n",
    "    elif row[\"State\"] == \"OCAP\":\n",
    "        return \"OCAP\"\n",
    "    else:\n",
    "        return \"DOWN\"\n",
    "\n",
    "\n",
    "\n",
    "def Transform_states(df,fo_row_id):\n",
    "    df[\"ReducedState\"] = df.apply(lambda row: ReducedState(row),axis=1)\n",
    "    Removedcolumns1 = df.drop(columns=[\"State\", \"fo_row_id\"])\n",
    "    RnmStateClmn = Removedcolumns1.rename(\n",
    "    columns={\n",
    "        \"ReducedState\": \"State\"\n",
    "    })\n",
    "    Sortedrows = RnmStateClmn.sort_values(by=[\"EVENT_ROW_ID\"])\n",
    "    index =  range(0,len(Sortedrows))\n",
    "    Sortedrows[\"Index\"] = index\n",
    "    State_DOWN_Remove = Sortedrows.drop(columns=[\"Datim\", \"Index\", \"EVENT_ROW_ID\"]).rename(\n",
    "    columns={\n",
    "        \"State\": \"ReducedState_DOWN\"\n",
    "    })\n",
    "    Sortedrows[\"ReducedState_DOWN\"] = Sortedrows[\"State\"].shift(1)\n",
    "    Sortedrows[\"Repeated\"] = np.where(Sortedrows[\"ReducedState_DOWN\"] == Sortedrows[\"State\"],True,False)\n",
    "    Repeated_State = Sortedrows.copy()\n",
    "    NoRepeats = Repeated_State[Repeated_State[\"Repeated\"] == False]\n",
    "    RemoveTmpClmns = NoRepeats.drop(columns=[\"Index\", \"ReducedState_DOWN\", \"Repeated\"])\n",
    "\n",
    "    if RemoveTmpClmns.shape[0] == 0:\n",
    "        Sortedrows[\"fo_row_id\"] = fo_row_id\n",
    "        return Sortedrows[[\"fo_row_id\",\"Datim\",\"EVENT_ROW_ID\",\"State\"]].iloc[0:1]\n",
    "    else:\n",
    "        RemoveTmpClmns[\"fo_row_id\"] = fo_row_id\n",
    "        return RemoveTmpClmns\n",
    "\n",
    "\n",
    "def RemoveRepeats(df):\n",
    "    RC = df.drop(columns=[\"ENT_NAME\",\"FACILITY\"])\n",
    "    sorted_rows = RC.sort_values([\"EVENT_ROW_ID\"])\n",
    "    index = range(0,len(sorted_rows))\n",
    "    sorted_rows[\"Index\"] = index\n",
    "    State_UP_Add_Index = sorted_rows.copy()\n",
    "    State_UP_Add_Index[\"Datim_UP\"] = State_UP_Add_Index[\"Datim\"].shift(-1)\n",
    "    State_UP_Add_Index[\"Datim_UP\"] = pd.to_datetime(State_UP_Add_Index[\"Datim_UP\"],format=\"%m/%d/%Y %H:%M\")\n",
    "    State_UP_Add_Index[\"Datim\"] = pd.to_datetime(State_UP_Add_Index[\"Datim\"],format=\"%m/%d/%Y %H:%M\")\n",
    "    ReplacedValue = State_UP_Add_Index.copy()\n",
    "    ReplacedValue[\"Datim_UP\"] = ReplacedValue[\"Datim_UP\"].fillna(datetime.now())\n",
    "    ReplacedValue[\"Duration_hrs\"] = (ReplacedValue[\"Datim_UP\"] - ReplacedValue[\"Datim\"])/np.timedelta64(1, 'h')\n",
    "    ReplacedValue[\"Duration_mins\"] = (ReplacedValue[\"Datim_UP\"] - ReplacedValue[\"Datim\"])/np.timedelta64(1, 'm')\n",
    "    FilteredRows2 = ReplacedValue[ReplacedValue[\"Duration_mins\"] > 10]\n",
    "\n",
    "    RemovedColumns1 = FilteredRows2.drop(columns=[\"Index\", \"Datim_UP\", \"Duration_hrs\",\"Duration_mins\"])\n",
    "    Sortedrows1 = RemovedColumns1.sort_values([\"EVENT_ROW_ID\"])\n",
    "    With_DOWN_Expanded = Sortedrows1.copy()\n",
    "    With_DOWN_Expanded[\"State_DOWN\"] = With_DOWN_Expanded[\"State\"].shift(1)\n",
    "    With_DOWN_Expanded[\"Repeated\"] = np.where(With_DOWN_Expanded['State']==With_DOWN_Expanded['State_DOWN'], True, False)\n",
    "    NoRepeats = With_DOWN_Expanded[With_DOWN_Expanded[\"Repeated\"] == False]\n",
    "    RemovedColumns2 = NoRepeats.drop(columns=[\"State_DOWN\", \"Repeated\"])\n",
    "    return RemovedColumns2[[\n",
    "        \"EVENT_ROW_ID\", \n",
    "         \"Datim\", \n",
    "         \"State\"\n",
    "    ]]\n",
    "\n",
    "\n",
    "def GetClusterStates(df):\n",
    "    SC = df[df[\"ENT_NAME\"].str.startswith(\"SC\")]\n",
    "    SC_rem_clmns = SC.drop(columns=[\"FACILITY\", \"ENT_NAME\", \"LithoCluster\"])\n",
    "    SC_renamed = SC_rem_clmns.rename(columns={\n",
    "        \"State\": \"State_SC\"\n",
    "    })\n",
    "\n",
    "\n",
    "    TR = df[df[\"ENT_NAME\"].str.startswith(\"TR\")]\n",
    "    TR_rem_clmns = TR.drop(columns=[\"FACILITY\", \"ENT_NAME\", \"LithoCluster\"])\n",
    "    TR_renamed = TR_rem_clmns.rename(columns={\n",
    "        \"State\": \"State_TR\"\n",
    "    })\n",
    "    TR_renamed\n",
    "\n",
    "    CombinedQueries = pd.concat([SC_renamed,TR_renamed])\n",
    "\n",
    "    SortedRows = CombinedQueries.sort_values([\"EVENT_ROW_ID\"])\n",
    "    FilledDown = SortedRows.copy()\n",
    "    FilledDown[\"State_SC\"] = FilledDown[\"State_SC\"].fillna(method='ffill')\n",
    "    FilledDown[\"State_TR\"] = FilledDown[\"State_TR\"].fillna(method='ffill')\n",
    "    FilledDown[\"State_SC\"] = FilledDown[\"State_SC\"].fillna('DOWN')\n",
    "    FilledDown[\"State_TR\"] = FilledDown[\"State_TR\"].fillna('DOWN')\n",
    "    AddedCustom1 = FilledDown.copy()\n",
    "    AddedCustom1[\"State\"] = np.where((AddedCustom1[\"State_SC\"] == \"UP\") & (AddedCustom1[\"State_TR\"] == \"UP\"),\"UP\",\"DOWN\")\n",
    "    AddedCustom1.drop(columns=[\"ToolName\"])\n",
    "    return AddedCustom1\n",
    "\n",
    "def Tools_states_material_suppliers():\n",
    "    #Tool_states = pd.read_csv(\"20221125\\Tool_States.csv\")\n",
    "    #Tools_Parents = pd.read_csv(\"20221125\\Tools_Parents.csv\")\n",
    "\n",
    "\n",
    "    Expanded_Tools_parents = tool_states.merge(\n",
    "        Tools_Parents, \n",
    "        how='inner',\n",
    "        left_on = [\"fo_row_id\"],\n",
    "        right_on = [\"ROW_ID\"],\n",
    "        indicator=True\n",
    "    )\n",
    "\n",
    "\n",
    "    Filtered_rows = Expanded_Tools_parents[\n",
    "        (Expanded_Tools_parents[\"ENT_NAME\"].notnull())\n",
    "        & (Expanded_Tools_parents[\"ENT_NAME\"] != \"\")\n",
    "    ]\n",
    "    Filtered_rows[\"State\"] = Filtered_rows.apply(lambda row: replaceStateValueSPC(row),axis=1)\n",
    "    Filtered_rows[\"State\"] = Filtered_rows.apply(lambda row: replaceStateValueOCAP(row),axis=1)\n",
    "\n",
    "    Removed_Columns = Filtered_rows.drop(columns=[\n",
    "        \"Area\", \n",
    "        \"fo_row_id\",\n",
    "        \"_merge\",\n",
    "        \"ROW_ID\"\n",
    "    ])\n",
    "\n",
    "    GroupedRows = pd.DataFrame(columns=[\n",
    "        \"FACILITY\",\n",
    "        \"ENT_NAME\",\n",
    "        \"EVENT_ROW_ID\", \n",
    "        \"Datim\", \n",
    "        \"State\"\n",
    "    ])\n",
    "    facilities = Removed_Columns[\"FACILITY\"].unique()\n",
    "\n",
    "    for facility in facilities:\n",
    "        facilitydata = Removed_Columns[Removed_Columns[\"FACILITY\"] == facility]\n",
    "        ent_names = facilitydata[\"ENT_NAME\"].unique()\n",
    "        \n",
    "        for ent_name in ent_names:\n",
    "            data = facilitydata[facilitydata[\"ENT_NAME\"] == ent_name]\n",
    "            NoRepeats = RemoveRepeats(data)\n",
    "            NoRepeats[\"FACILITY\"] = facility\n",
    "            NoRepeats[\"ENT_NAME\"] = ent_name\n",
    "            \n",
    "            GroupedRows = pd.concat([GroupedRows,NoRepeats])\n",
    "            \n",
    "\n",
    "    MergedQueries = pd.merge(\n",
    "            GroupedRows, \n",
    "            LithoClusters, \n",
    "            left_on=[\"ENT_NAME\"], \n",
    "            right_on=[\"ToolName\"], \n",
    "            how=\"left\",\n",
    "            suffixes=[\"\",\"_y\"]\n",
    "        )\n",
    "\n",
    "\n",
    "    ClusterStates = MergedQueries[(MergedQueries[\"LithoCluster\"].notnull()) & (MergedQueries[\"LithoCluster\"] != \"\")]\n",
    "\n",
    "\n",
    "    GrClusterStates = pd.DataFrame(columns=[\n",
    "        'EVENT_ROW_ID', \n",
    "        'Datim', \n",
    "        'State_SC', \n",
    "        'State_TR', \n",
    "        'State',\n",
    "        'FACILITY',\n",
    "        'LithoCluster'\n",
    "        \n",
    "    ])\n",
    "\n",
    "    facilities = ClusterStates[\"FACILITY\"].unique()\n",
    "\n",
    "    for facility in facilities:\n",
    "        facilitydata = ClusterStates[Removed_Columns[\"FACILITY\"] == facility]\n",
    "        Litho_Clusters = facilitydata[\"LithoCluster\"].unique()\n",
    "        \n",
    "        for LithoCluster in Litho_Clusters:\n",
    "            data = facilitydata[facilitydata[\"LithoCluster\"] == LithoCluster]\n",
    "            ClusterStatesData = GetClusterStates(data)\n",
    "            ClusterStatesData[\"FACILITY\"] = facility\n",
    "            ClusterStatesData[\"LithoCluster\"] = LithoCluster\n",
    "            \n",
    "            GrClusterStates = pd.concat([GrClusterStates,ClusterStatesData])\n",
    "            print(facility,LithoCluster)\n",
    "            \n",
    "\n",
    "    GrClusterStates = pd.DataFrame(columns=[\n",
    "        'EVENT_ROW_ID', \n",
    "        'Datim', \n",
    "        'State_SC', \n",
    "        'State_TR', \n",
    "        'State',\n",
    "        'FACILITY',\n",
    "        'LithoCluster'\n",
    "        \n",
    "    ])\n",
    "\n",
    "    facilities = ClusterStates[\"FACILITY\"].unique()\n",
    "\n",
    "    for facility in facilities:\n",
    "        facilitydata = ClusterStates[Removed_Columns[\"FACILITY\"] == facility]\n",
    "        Litho_Clusters = facilitydata[\"LithoCluster\"].unique()\n",
    "        \n",
    "        for LithoCluster in Litho_Clusters:\n",
    "            data = facilitydata[facilitydata[\"LithoCluster\"] == LithoCluster]\n",
    "            ClusterStatesData = GetClusterStates(data)\n",
    "            ClusterStatesData[\"FACILITY\"] = facility\n",
    "            ClusterStatesData[\"LithoCluster\"] = LithoCluster\n",
    "            \n",
    "            GrClusterStates = pd.concat([GrClusterStates,ClusterStatesData])\n",
    "            print(facility,LithoCluster)\n",
    "            \n",
    "\n",
    "    GrExpand = GrClusterStates.copy()\n",
    "    GrExpand[\"LithoCluster\"] = [\"LithoCluster_\" + str(x) for x in GrExpand[\"LithoCluster\"]]\n",
    "    GrReplace = GrExpand.copy()\n",
    "    GrRename = GrReplace.rename(columns={\n",
    "        \"LithoCluster\":\"Ent_Name\"\n",
    "    })\n",
    "    TrStates = GrRename.copy()\n",
    "    TrStates[\"Ent_Name\"] = TrStates[\"Ent_Name\"].str.replace(\"LithoCluster_\",\"TR\")\n",
    "    TrStates[\"State\"] = TrStates[\"State_TR\"]\n",
    "    NonClusters = MergedQueries[MergedQueries[\"LithoCluster\"].isnull()]\n",
    "    NonClustersRC = NonClusters.drop(columns=[\"LithoCluster\"])\n",
    "    Combined = pd.concat([NonClustersRC, GrRename, TrStates])\n",
    "    Combined = Combined.rename(columns={\"Ent_Name\": \"Tool\"})\n",
    "    return Combined\n",
    "        \n",
    "\n",
    "\n",
    "def replaceStateValueSPC(row):\n",
    "    if (\"ILIME\" in row[\"Area\"]) and (row[\"ENT_NAME\"] in AllTracks):\n",
    "        return row[\"State\"].replace(\"SPC\",\"UP\")\n",
    "    return  row[\"State\"].replace(\"SPC\",\"DOWN\")\n",
    "\n",
    "def replaceStateValueOCAP(row):\n",
    "    if (row[\"ENT_NAME\"] in AllTracks):\n",
    "        return row[\"State\"].replace(\"OCAP\",\"UP\")\n",
    "    return  row[\"State\"].replace(\"OCAP\",\"DOWN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aef870f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Fab300_Res_id', 'IIO_Res_id', 'Cluster', 'Fab300_Begin', 'Fab300_End',\n",
      "       'FACILITY', 'ResTk', 'Tool', 'Fab300_User_id', 'WBS', 'IIO_Begin',\n",
      "       'Description', 'IIO_End', 'FO_ROW_ID', 'EVENT_ROW_ID_Begin', 'Modules',\n",
      "       'IIO_User_id'],\n",
      "      dtype='object')\n",
      "MergedQueries (1531, 14)\n",
      "PLINE300 1000.0\n",
      "PLINE300 1950.0\n",
      "PLINE300 3400.0\n",
      "PLINE300 3300.0\n",
      "PLINE300 1000.0\n",
      "PLINE300 1950.0\n",
      "PLINE300 3400.0\n",
      "PLINE300 3300.0\n",
      "                    Datim EVENT_ROW_ID State  fo_row_id\n",
      "0     2022-09-01 00:02:00    929363260  DOWN      233.0\n",
      "3118  2022-09-05 11:01:00    930424140    UP      233.0\n",
      "57370 2022-11-15 09:22:00    948601785  DOWN      233.0\n",
      "57452 2022-11-15 10:23:00    948616564    UP      233.0\n",
      "60162 2022-11-17 23:28:00    949460594  DOWN      233.0\n"
     ]
    }
   ],
   "source": [
    "#Start of execution\n",
    "folder = \"C:\\\\Users\\\\fpicaso\\\\Repos\\\\PMOPS\\\\20221125\\\\\"\n",
    "Fab300_Raw = pd.read_csv(folder + \"5. Fab300 Raw Reservations.csv\")\n",
    "Tools_with_reservations = processFab300RawReservations(Fab300_Raw)\n",
    "\n",
    "Tools_Parents = pd.read_csv(folder + \"Tools_Parents.csv\")\n",
    "Tools_Parents[\"CSIM_TIMESTAMP\"] = pd.to_datetime(Tools_Parents[\"CSIM_TIMESTAMP\"])\n",
    "\n",
    "df_FAB300_with_tool_names = FAB300_with_tool_names(Tools_with_reservations,Tools_Parents)\n",
    "\n",
    "\n",
    "IIO_raw = pd.read_csv(folder + \"4. IIO_raw_reservations .csv\")\n",
    "df_IIO_without_modules = IIO_without_modules(IIO_raw)\n",
    "\n",
    "\n",
    "#Fab300_IIO_overlaps_ids\n",
    "Source_fab = df_FAB300_with_tool_names.copy()\n",
    "RC_fab = Source_fab.drop(columns=[\"ResTk\", \"User_id\"])\n",
    "UnPivot_fab = pd.melt(RC_fab, id_vars=[\"WBS\", \"FACILITY\", \"Tool\", \"Fab300_Res_id\"], \n",
    "                value_vars=[\"Begin\", \"End\"])\n",
    "UnPivot_fab = UnPivot_fab.rename(\n",
    "columns={\n",
    "    \"variable\": \"FAB300_BeginEnd\",\n",
    "    \"value\": \"DateTime\"\n",
    "})\n",
    "\n",
    "Source_iio = df_IIO_without_modules.copy()\n",
    "RC_iio = Source_iio.drop(columns=[\"Modules\", \"User_id\", \"Description\"])\n",
    "UnPivot_iio = pd.melt(RC_iio, id_vars=[\"WBS\", \"FACILITY\", \"Tool\", \"IIO_Res_id\"], \n",
    "                value_vars=[\"Begin\", \"End\"])\n",
    "\n",
    "UnPivot_iio = UnPivot_iio.rename(\n",
    "columns={\n",
    "    \"variable\": \"IIO_BeginEnd\",\n",
    "    \"value\": \"DateTime\"\n",
    "})\n",
    "\n",
    "fab_iio_together  = pd.concat([UnPivot_fab, UnPivot_iio], ignore_index=True)\n",
    "\n",
    "columns = [\n",
    "    'Fab300_Res_id', \n",
    "    'IIO_Res_id', \n",
    "    'Cluster', \n",
    "    'Fab300_Begin', \n",
    "    'Fab300_End',\n",
    "    'FACILITY', \n",
    "    'ResTk', \n",
    "    'Tool', \n",
    "    'Fab300_User_id', \n",
    "    'WBS', \n",
    "    'IIO_Begin',\n",
    "    'Description', \n",
    "    'IIO_End'\n",
    "]\n",
    "\n",
    "Fab300_IIO_overlaps_ids = pd.DataFrame(columns = columns)\n",
    "\n",
    "WBSs = fab_iio_together[\"WBS\"].unique()\n",
    "\n",
    "for wbs in WBSs:\n",
    "    wbsdata = fab_iio_together[\n",
    "        (fab_iio_together[\"WBS\"] == wbs)\n",
    "    ]\n",
    "    facilities = wbsdata[\"FACILITY\"].unique()\n",
    "    for facility in facilities:\n",
    "        facilityData = wbsdata[\n",
    "            (wbsdata[\"FACILITY\"] == facility)\n",
    "        ]\n",
    "        \n",
    "        tools = facilityData[\"Tool\"].unique()\n",
    "        for tool in tools:\n",
    "            wbstooldata = facilityData[\n",
    "                (wbsdata[\"Tool\"] == tool)\n",
    "            ]\n",
    "            df = Fab300_iio_merger(wbstooldata)\n",
    "            Fab300_IIO_overlaps_ids = pd.concat([Fab300_IIO_overlaps_ids,df])\n",
    "            \n",
    "            \n",
    "\n",
    "#Final_Fab300_IIO_reservations\n",
    "\n",
    "Final_Fab300_IIO_reservations_df = Final_Fab300_IIO_reservations(df_IIO_without_modules,df_FAB300_with_tool_names,Fab300_IIO_overlaps_ids)\n",
    "\n",
    "\n",
    "#Step 14\n",
    "Final_Fab300_IIO_reservations_df[\"Begin\"] = pd.to_datetime(Final_Fab300_IIO_reservations_df[\"Begin\"],format=\"%Y-%m-%d %H:%M\")\n",
    "Final_Fab300_IIO_reservations_df[\"End\"] = pd.to_datetime(Final_Fab300_IIO_reservations_df[\"End\"],format=\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "\n",
    "LithoClusters = pd.read_csv(folder + \"LithoClusters.csv\")\n",
    "\n",
    "Final_Fab300_IIO_Reservations_clustered_df = Final_Fab300_IIO_Reservations_clustered(Final_Fab300_IIO_reservations_df)\n",
    "\n",
    "\n",
    "#Tool States and Transform States\n",
    "\n",
    "tool_states = pd.read_csv(folder + \"Tool_States.csv\")\n",
    "tool_states[\"Datim\"] = pd.to_datetime(tool_states[\"Datim\"],format=\"%m/%d/%Y %H:%M\")\n",
    "\n",
    "\n",
    "fo_row_ids = tool_states[\"fo_row_id\"].unique()\n",
    "\n",
    "tool_states_result = pd.DataFrame(\n",
    "columns=[\n",
    "    \"Datim\",\n",
    "    \"EVENT_ROW_ID\",\n",
    "    \"State\"\n",
    "])\n",
    "\n",
    "for fo_row_id in fo_row_ids:\n",
    "    df = tool_states[tool_states[\"fo_row_id\"] == fo_row_id]\n",
    "    tool_states_df = Transform_states(df,fo_row_id)\n",
    "    tool_states_result = pd.concat([tool_states_result,tool_states_df])\n",
    "\n",
    "\n",
    "\n",
    "#17. Tools_states_material_suppliers\n",
    "AllTracks = [\"TR1000\", \"TR1950\", \"TR1970\", \"TR2000\", \"TR3300\", \"TR3400\", \"TRDSA\", \"TRMTM\", \"TRDUOS\"]\n",
    "\n",
    "Tools_states_material_suppliers_df = Tools_states_material_suppliers()\n",
    "\n",
    "print(tool_states_result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f639174",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AddedCustom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18404/7418499.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mAddedCustom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'AddedCustom' is not defined"
     ]
    }
   ],
   "source": [
    "AddedCustom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e8b9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
